# 线性模型

线性模型是一种非概率模型，它试图学得一个通过属性的**线性组合**来进行预测的函数

$$
f(\pmb x) = \pmb\omega^T \pmb x + b
$$

其中 $\pmb\omega = (\omega_1;\omega_2;\cdots;\omega_d)$ 为参数向量，$\pmb x$ 为特征向量。

## 线性回归

* 线性回归的一维形式在中学阶段已经学过，但是当时多半只给了个结论。

对于一维输入的线性模型

$$
f(x_i) = \omega x_i + b
$$

我们希望平方损失函数（或欧式距离）

$$
L(Y,f(X);\omega, b) = \sum_{i = 1}^m(f(x_i) - y_i)^2
$$

达到最小，使得模型对真实情况 $y_i$ 有良好的拟合

$$
(\omega^*, b^*) = \underset{(\omega, b)}{\arg\min}\sum_{i=1}^m(f(x_i) - y_i)^2
$$

这个式子的意思是找到一个 $(\omega, b) = (\omega^*, b^*)$ 使得训练结果与真实值的欧式距离之和最小。

对 $\omega, b$ 求偏导

$$
\dfrac{\partial L}{\partial\omega} = 2\left(\omega\sum_{i=1}^m x^2_i - \sum_{i=1}^m(y_i - b)x_i\right)
$$

$$
\dfrac{\partial L}{\partial b} =2\left( mb - \sum_{i = 1}^m(y_i - \omega x_i) \right)
$$

取偏导为 $0$ 求极值解得

$$
\omega = \dfrac{\sum_{i=1}^m y_i(x_i - \overline{x})}{\sum_{i=1}^m x_i^2 - \frac{1}{m}(\sum_{i=1}^m x_i)^2}
$$

$$
b = \dfrac{1}{m}\sum_{i=1}^m(y_u - \omega x_i)
$$

* 对于多元线性回归，需要用到矩阵求导（因为变量 $\pmb\omega$ 是一个向量，所以其实只是对一个列向量求导罢了），大体上与正常求导一样，说几点重要的。

>（1）矩阵求导的四则运算法则与实函数的求导法则形式相同
>
>（2）矩阵求导和实函数求导一样具有线性性质
>
>（3）关于转置矩阵有性质
>
>$$
\dfrac{\partial \pmb x^T \pmb a}{\partial\pmb x}=\dfrac{\partial\pmb a^T\pmb x}{\partial\pmb x}=\pmb a
$$
>
>$$
\dfrac{\partial\pmb x^T\pmb x}{\partial\pmb x}=2\pmb x
$$
>
>$$
>\dfrac{\partial\pmb x^T \pmb A \pmb x}{\partial\pmb x}=\pmb{Ax} + \pmb A^T \pmb x
>$$
>
>

有关以上性质的证明和矩阵求导参见[矩阵求导公式的数学推导](https://zhuanlan.zhihu.com/p/273729929)（恐怖如斯的 $\LaTeX$ 公式）。

同样的，对于多元线性回归的情况

$$
f(\pmb x_i) = \pmb\omega^T \pmb x_i + b
$$

平方损失函数可以写成

$$
\begin{aligned}
    L(Y,f(X);\pmb\omega, b)
    &= \sum_{i = 1}^m(f(\pmb x_i) - y_i)^2\\
    &= \sum_{i = 1}^m(\pmb\omega^T \pmb x_i + b - y_i)^2\\
\end{aligned}
$$

不难发现其中有一实数 $b$ 导致难以处理，故我们设一 $\hat{\pmb\omega} = (\pmb \omega; b)$，并令 $\pmb y = (y_1; y_2;\cdots; y_d)$，且

$$
\textbf X = 
\begin{bmatrix}
    x_{11} & x_{12} &\cdots& x_{1d} & 1\\
    x_{11} & x_{12} &\cdots& x_{1d} & 1\\
    \vdots & \vdots &\ddots& \vdots & \vdots\\
    x_{d1} & x_{d2} &\cdots& x_{dd} & 1\\
\end{bmatrix}
= \
\begin{bmatrix}
   \pmb x_1^T & 1\\
   \pmb x_2^T & 1\\
   \vdots & \vdots\\
   \pmb x_d^T & 1\\
\end{bmatrix}
$$

则有

$$
L(Y,f(X);\pmb\omega, b) = (\pmb y - \textbf X\hat{\pmb\omega})^T(\pmb y - \textbf X\hat{\pmb\omega})
$$

对 $\hat{\pmb\omega}$ 求导（求导第三条性质）得到

$$
\dfrac{\partial L}{\partial{\hat{\pmb\omega}}} = 2\textbf X^T(\textbf X\hat{\pmb\omega} - \pmb y)
$$

当 $\textbf X^T\textbf X$ 为满秩矩阵时，存在逆矩阵 $(\textbf X^T\textbf X)^{-1}$，此时 $\dfrac{\partial L}{\partial{\hat{\pmb\omega}}} = 0$ 有解

$$
\hat{\pmb\omega}^* = (\textbf X^T\textbf X)^{-1}\textbf X^T y
$$

令 $\hat{\pmb x_i} = (\pmb x_i;1)$ 则此线性回归模型的最小二乘近似为

$$
f(\hat{\pmb x_i}) = \hat{\pmb x_i}^T\hat{\pmb\omega}^* = \hat{\pmb x_i}^T(\textbf X^T\textbf X)^{-1}\textbf X^T y
$$

但一般情况下，$\textbf X^T\textbf X$ 非满秩，存在多个解需要进行迭代优化，而随机梯度下降或牛顿迭代法可能遭遇多个极小值，这时就需要引入正则项判断解的优良性。

### 线性回归的求解方法

#### 最小二乘法

* 最小二乘法通常被认为是一种解析解直接求解法，但是实际上它意味着一种特定的最优化方法，即对模型平方损失的最小化，它实际上完全可以被看作一种设计损失函数的思路，即平方损失函数。或者说，**最小二乘法的思路不一定能够给出解析解，但是一定可以给出一种利用导数为零的思想去寻找极小值的方法**。

如同上面提到的，最小二乘法对于一组输入训练集

$$
\{\pmb x_i, y_i\}_{i = 1}^m
$$

假设它满足线性模型

$$
f(\pmb x_i) = \pmb\omega^T \pmb x_i + b
$$

并希望计算得到参数 $(\pmb\omega, b)$ 将以下平方损失函数优化至最小，使得模型对真实情况的拟合程度最高

$$
\begin{aligned}
    L(Y,f(X);\pmb\omega, b)
    &= \sum_{i = 1}^m(f(\pmb x_i) - y_i)^2\\
    &= \sum_{i = 1}^m(\pmb\omega^T \pmb x_i + b - y_i)^2\\
\end{aligned}
$$

但是实际上，这个问题一般有多种解法

* 解析求解法：令偏导数为零解矩阵方程，如果数据量较大的话计算速度非常缓慢，尤其是逆矩阵的计算。
* 迭代求解法：常见的比如**梯度下降法和牛顿迭代法**，实际上都是求解极值点的具体方法。

先看解析求解法，迭代求解法单独说。

很简单，直接写个表达式就完了。

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

train_buf = pd.read_csv('train.csv')
test_buf = pd.read_csv('test.csv')

train = np.array(train_buf)
test = np.array(test_buf)

x = train[:, 0]
y = train[:, 1]

avg_x = np.average(x)

# 就这
omega_0 = np.sum(y * (x - avg_x)) / (np.sum(x ** 2) - (np.sum(x) ** 2 / np.shape(x)[0]))
b_0 = np.sum(y - omega_0 * x) / np.shape(x)[0]

print('ω: ', omega_0, 'b: ', b_0)

plt.title('Result of Agression')

plt.xlabel('x')
plt.ylabel('y')

plt.scatter(x, y)

x_0 = np.linspace(0, 3)
plt.plot(x_0, omega_0 * x_0 + b_0)
plt.grid()

plt.show()
```


#### 参数估计法

* 参数估计法是一种依托于概率论的方法（你甚至可以在一些概率的书上看到这玩意），一般也就是概率模型用一下极大似然估计之类的。参数估计法通常指的就是朴素贝叶斯，本质上是在通过找一组特定的参数来做一个**最大化后验概率**（给定输入的条件概率）的事情，或者说调整模型的参数使之尽可能地逼近真实的输入与输出的联合概率分布（一般来说我们无法得知这个分布究竟为何种形式）。

搞参数估计，就不得不提极大似然估计，因为这个东西本质上就是个最优化问题嘛，交给计算机去迭代求解最好了。

我们要做的事情很简单，就是对给定输入训练集

$$
\{\pmb x_i, y_i\}_{i = 1}^m
$$

假设它满足概率模型（某些线性模型可以被当成概率模型考虑，待会会看见）

$$
\{P|P(y_i | x_i)\}
$$

那么构造对数似然函数

$$
L(\pmb x_i;\theta) = \sum_{i = 1}^m\ln P(y_i|x_i;\theta)
$$

使得 $L$ 最大的 $\theta$ 就是我们要找的参数，这个函数最优化任务就交给迭代算法完成了。

> 注意：在实际的机器学习算法中，我们一般最优化一个损失函数使得它最小化，所以一般来说我们需要处理一下似然函数把它变成损失函数来做最优化。
> $$
J(\pmb x_i;\theta) = -\dfrac{1}{m}\sum_{i = 1}^m\ln P(y_i|x_i;\theta)
$$
> 这个就是实际要优化的损失函数了。

#### 梯度下降法

> 注意：梯度下降法和牛顿迭代法其实与上面两种方法不是同级的，单纯是因为它们比较重要所以单独拿出来写一点东西。

高等数学告诉我们[梯度](../../高等数学（下）/第六章/方向导数与梯度.md#梯度)的方向是方向导数最大的方向，即函数值变化最快的方向，那么沿着它的反方向我们便可以以最速下降找到一个局部极小值，因为这是一个贪心，所以往往结果不是最优的，这个一般通过正则化解决。

先解决一维输入的线性模型

$$
f(x_i) = \omega x_i + b
$$

它的平方损失函数

$$
L(Y,f(X);\omega, b) = \sum_{i = 1}^m(f(x_i) - y_i)^2
$$

对 $\omega, b$ 求偏导

$$
\dfrac{\partial L}{\partial\omega} = 2\sum_{i = 1}^mx_{i}(wx_{i}+b-y_i)
$$

$$
\dfrac{\partial L}{\partial b} =2\sum_{i = 1}^m(wx_i+b-y_i)
$$

那梯度就是

$$
\operatorname{grad} L|_{(\omega, b)} = (\dfrac{\partial L}{\partial \omega}, \dfrac{\partial L}{\partial b})
$$

* 梯度下降法的核心思想是**迭代**，形象地说就是往负梯度方向一小步一小步地在函数曲线上移动直到迭代次数达到某一上限或参数更新的幅度小于某个阈值。

所以更新公式是这俩

$$
w\gets w-\frac{\eta}{m}\sum_{i = 1}^mx_{i}(wx_{i}+b-y_i)
$$

$$
b\gets b-\frac{\eta}{m}\sum_{i = 1}^m(wx_i+b-y_i)
$$

其中 $\eta$ 称为**学习率**，或者形象地理解成步长，$m$ 是样本数量。

贴上代码（代码实现是批量随机梯度下降）

> 注：梯度下降又可细分为批量随机梯度下降、小批量随机梯度下降和批量梯度下降，这个顾名思义就好，无非就是抽取样本的手段不同罢了。

```python
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

train_buf = pd.read_csv('train.csv')
test_buf = pd.read_csv('test.csv')

train = np.array(train_buf)
test = np.array(test_buf)

x = train[:, 0]
y = train[:, 1]

eta = 0.08
size_of_sample = np.shape(x)[0]
argv = eta / size_of_sample

omega_1 = 0
b_1 = 0

step_max = 10000

for step in range(step_max):
    omega_ex = np.sum([x_0 *(omega_1 * x_0 + b_1 - y_0) for x_0, y_0 in train if np.random.randn() < 0.7])
    b_ex = np.sum([(omega_1 * x_0 + b_1 - y_0) for x_0, y_0 in train if np.random.randn() < 0.7])

    if(abs(omega_ex) < 0.1 and abs(b_ex) < 0.05): 
        break

    omega_1 = omega_1 - argv * omega_ex
    b_1 = b_1 - argv * b_ex
```

这样的话多维的情形也明朗了起来，让我们略过一部分上面已经讲过的内容直接写出多维输入的线性模型

$$
L(Y,f(X);\pmb\omega, b) = (\pmb y - \textbf X\hat{\pmb\omega})^T(\pmb y - \textbf X\hat{\pmb\omega})
$$

还有它的的导数

$$
\dfrac{\partial L}{\partial{\hat{\pmb\omega}}} = 2\textbf X^T(\textbf X\hat{\pmb\omega} - \pmb y)
$$

所以多维输入线性模型的梯度是什么？

首先根据矩阵求导的定义（按列逐个求导）直接可得

$$
\dfrac{\partial L}{\partial \omega_j} = 2\sum_{i = 1}^mx_{ij}(\omega_0 + \omega_1x_1+\omega_2x_2+\cdots+\omega_mx_m-y_i),~j = 0,1,2\cdots,m
$$


推广一下低维梯度的情形很简单得可以得到

$$
\operatorname{grad} ~L|_{(\pmb{\hat\omega}, b)} = (\dfrac{\partial L}{\partial \omega_0}, \dfrac{\partial L}{\partial \omega_1}, \cdots, \dfrac{\partial L}{\partial \omega_m})
$$

所以更新公式就可以写成

$$
\omega_j \leftarrow \omega_j - \dfrac{\eta}{m}\sum_{i = 1}^mx_{ij}\left(\sum_{k = 0}^m\omega_kx_{ik}-y_i\right),j = 0,1,2,\cdots,m
$$

其中如 $x_{ij}$ 表示第 $i$ 个样本的第 $j$ 个输入变量。

贴上代码（也是个批量随机梯度下降，以及输入是三维的，不过实际上多少维都是一样的）

```python
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

train_buf = pd.read_csv('train2.csv')
test_buf = pd.read_csv('test2.csv')
train = np.array(train_buf)
test = np.array(test_buf)
train_ = np.hstack((np.ones((np.shape(train)[0], 1), dtype = float), train[:, :]))
size_ = 30

x = test[:, :3]
y = test[:, -1]
hat_x = np.hstack((np.ones((np.shape(test)[0], 1), dtype = float), test[:, :3]))

eta = 0.005
size_of_sample = np.shape(x)[0]
argv = eta / size_of_sample
step_max = 10000

omega_2 = np.zeros(4, float)

for step in range(step_max):
    omega_ex = [np.sum([point[j] * (np.sum(omega_2 * point[:4]) - point[-1]) for point in train_ if np.random.randn() < 0.4]) for j in range(4)]

    if(abs(np.sum(omega_ex)) < 0.02): 
        print('stop!')
        break

    omega_2 = omega_2 - np.array(argv) * omega_ex
```

#### 牛顿迭代法

* 牛顿迭代法和梯度下降法的内在思想是一致的，但是牛顿迭代法不仅用了一阶导，还用了二阶。

先说明一个问题以便于从梯度下降到牛顿迭代法的推广——**梯度下降法是否在数学上精确？** 答案是否定的。

梯度下降法每次向负梯度的方向移动一定步长，而损失函数值的变化取决于梯度值。实际上，这是**一阶泰勒展开**的方法，它的本质是**在某点的邻域将函数视为线性**，并在这个线性函数上改变参数值使得损失函数值减小。对于 $d + 1$ 维变量 $\pmb{\hat\omega} = (\omega;b)$，有

$$
\begin{aligned}
L(\pmb {\hat\omega}) 
&= L(\pmb {\hat\omega_0}) + \nabla L(\pmb {\hat\omega_0})^T(\pmb {\hat\omega} - \pmb {\hat\omega}_0) + o(\nabla L(\pmb {\hat\omega}_0))\\
&\approx L(\pmb {\hat\omega}_0) + \nabla L(\pmb {\hat\omega}_0)^T(\pmb {\hat\omega} - \pmb {\hat\omega}_0)\\
&\approx L(\pmb {\hat\omega}_0) + \sum_{i = 1}^{d + 1}\left.\dfrac{\partial L({\hat\omega}^{(i)})}{\partial {\hat\omega}^{(i)}}\right|_{{\hat\omega} = {\hat\omega}_0}({\hat\omega}^{(i)} - {\hat\omega}_0^{(i)})
\end{aligned}
$$

可以看到这个近似里面，只有 $(\pmb x - \pmb x_0)$ 是变量，所以这个近似是线性的。

在具体的算法中，我们规定了所有维度的单步步长 $\eta$，对于任意的第 $i$ 维，这其实就是 $(x^{(i)} - x_0^{(i)})$ 的一个规定值，每步我们向这个近似的线性函数下降的方向挪动一点点。

* 但是，这种方法有一个缺陷——**它的步长是固定的**，当损失函数变化缓慢时，它的效率十分低下。而牛顿迭代法在一定程度上解决了这个问题，它的步长是启发式的。

牛顿迭代法使用了更多的信息，即二阶导数。它首先对损失函数作二阶泰勒展开

$$
\begin{aligned}
    L(\pmb {\hat\omega}) 
    &= L(\pmb {\hat\omega}_0) + \nabla L(\pmb {\hat\omega}_0)^T(\pmb {\hat\omega} - \pmb {\hat\omega}_0) + \dfrac{1}{2}(\pmb {\hat\omega} - \pmb {\hat\omega}_0)^TH(\pmb{\hat\omega}_0)(\pmb{\hat\omega} - \pmb{\hat\omega}_0) + o(\nabla H(\pmb {\hat\omega}_0))
\end{aligned}
$$

其中 $H$ 为 $Hessian$ 矩阵，是一个 $(d + 1)\times (d + 1)$ 的矩阵，里面是 $L$ 的所有二阶导。

$$
\nabla L(\pmb{\hat\omega}) = \left[\dfrac{\partial L}{\partial \pmb{\hat\omega}}\right]_{(d + 1)\times 1},H(\pmb{\hat\omega}) = \left[\dfrac{\partial^2 L}{\partial \pmb{\hat\omega}_i\partial\pmb{\hat\omega}_j}\right]_{(d + 1) \times (d + 1)}
$$

用与梯度下降同样的手法分析，不难发现二阶泰勒展开把损失函数在 $\pmb\omega_0$ 的邻域**近似成了一个二次函数**，梯度决定了该点在这一近似函数关于最低点的偏移，而 $Hessian$ 矩阵决定了这个近似函数的形状。

与梯度下降不同的是，牛顿迭代法每次**将参数改动到这个近似的二次函数的最小值点的位置**（二次函数是凸函数）。

那么为什么上面说到牛顿迭代法是启发式的呢？因为这个方法每次挪动的距离是根据梯度和 $Hessian$ 矩阵的值变化的，而且是以符合我们直觉那样地变化的。

我们取一维输入形式下，$\omega_0 = 0$ 的情形分析，为了方便起见，此处不进行具体数值的讨论。

对于 $\omega_0 = 0$ 处不同的一阶导，它们将影响 $\omega_0$ 相对二次函数最低点的偏移量。

> 注：绿色的函数表示绝对值较小的一阶导。

![function3.png](http://image.tjzfile.xyz/images/2022/09/25/function3.png)

对于 $\omega_0 = 0$ 处不同的二阶导，它们将影响 $\omega_0$ 的形状。

> 注：绿色的函数表示绝对值较小的二阶导。

![funciton4.png](http://image.tjzfile.xyz/images/2022/09/25/funciton4.png)
![function5.png](http://image.tjzfile.xyz/images/2022/09/25/function5.png)

可以发现，他们都会影响步长。一阶导体现了函数的变化率，一阶导的绝对值越大函数越陡，近似的二次函数的最小值离 $\omega_0$ 越远，此时就增大步长。而二阶导的绝对值越大，函数越有可能在附近变缓/陡，那么此时就应该缩小步长。

也就是说，牛顿法是这样看待一个函数的：如果函数在此处变化越平缓，那么此处越有可能接近极小值。这与我们的直觉是一致的。

## 广义线性回归

* 通过对线性模型 $y = \pmb \omega^T\pmb x + b$ 的非线性变换，我们可以将线性回归转换成非线性回归，并使用线性回归的方法解决（反之依然）。

建立一个线性模型到非线性模型的映射 $g(\cdot)$，称为关联函数，令

$$
y' = g^{-1}(\pmb \omega^T\pmb x + b)
$$

即可得到一个非线性模型 $y'$，这个 $y'$ 可以通过 $g(y') = \pmb \omega^T\pmb x + b$ 转换成线性模型预测和拟合。

比如当 $g(\cdot) = \ln(\cdot)$ 时，线性模型变为对数线性回归 

$$
\ln y = \pmb \omega^T\pmb x + b
$$

这样一来，**就可以方便地对各种非线性模型应用最优化方法了**。

### 对数几率回归

* 对数几率回归说是回归，其实是用来分类的（准确地说是用类似回归的方法做分类），其中对数几率四字揭示了拟合函数的性质。

正常来想，用数值函数做二分类，当然是用二值函数之类的，像阶跃函数这种：

$$
y = \left\{
\begin{aligned}
    0,\ z < 0\\
    0.5,\ z = 0\\
    1,\ z > 0
\end{aligned}
\right.
\\\ \\
z = \pmb \omega^T\pmb x + b
$$

然后做一个从线性模型到这个二值（或者应该是三值）函数的映射，就可以容易地解决了。

但是实际上这类函数就难以做数值优化，或者说难以放进机器学习里面去用，因为它**不连续**，没法迭代。于是我们考虑一形状十分相似的函数

$$
y = \dfrac{1}{1+ e^{-z}}
$$

![geogebra-export2.png](http://image.tjzfile.xyz/images/2022/09/17/geogebra-export2.png)

它具有极其优秀的性质（这也注定了它在机器学习中的广泛应用）：

* 无穷阶可微
* 处处连续
* 单调递增且有界

带入 $z$ 并整理一下得到

$$
\ln\dfrac{y}{1 - y} = \pmb \omega^T\pmb x + b
$$

我们要做线性回归，拟合的就是左边这个函数。但是实际上不难发现，由于标记 $\{y_i\}_{i = 1}^m\in \{0, 1\}$ 的取值导致分母有可能为零，这就使得直接的（即设 $y' = \ln\dfrac{y}{1 - y}$）线性回归方法变得不可行。怎么办？

这个 $\dfrac{y}{1 - y}$ 就可以做一些有趣的解读了，我们不妨假设 $y$ 代表 $\pmb x$ 为正例的概率，而 $1 - y$ 代表反例的（正是得益于这种函数的单调连续有界性使得我们能够这样假设），那么则称 $\dfrac{y}{1 - y}$ 为[**几率**](https://developers.google.com/machine-learning/glossary#log-odds)，取对数就是对数几率，这就是这一方法名字的由来。

> 注：
> （1）实际上，在深度神经网络中，如果将线性回归模型作为一层， $\delta(z) = \dfrac{1}{1 - e^{-z}}$ 视为激活函数，那么这个问题是不存在的。
> （2）对数几率其实是交叉熵损失在二维输入时的特例。

根据上面的假设，我们可以作如下规定

$$
y = p(y = 1|\pmb x),1 - y = p(y = 0|\pmb x)
$$

可得

$$
p(y = 1|\pmb x) = \dfrac{e^{\pmb \omega^T\pmb x + b}}{1 + e^{\pmb \omega^T\pmb x + b}}
$$

$$
p(y = 0|\pmb x) = \dfrac{1}{1 + e^{\pmb \omega^T\pmb x + b}}
$$

作这样的假设，其实是为了寻求概率论的方法来进行参数估计，将离散值通过与参数的结合映射到某个概率上，然后采用极大似然估计法设计损失函数即可。

> 注：在后续的深度网络中将提供更多解决方案。

$$
\begin{aligned}
    L(\pmb \omega, b) 
    &= \sum_{i = 1}^m\ln p(y_i|\pmb x_i; \pmb \omega, b)\\
    &= \sum_{i = 1}^m\ln p(y_i = 1|\pmb x_i)^{y_i}p(y_i = 0|\pmb x_i)^{1 - y_i}\\
    &= \sum_{i = 1}^m[y_i(\pmb \omega^T\pmb x+ b) - \ln(1 + e^{\pmb \omega^T\pmb x+ b})]\\
    &= -\sum_{i = 1}^m[-y_i(\pmb \omega^T\pmb x+ b) + \ln(1 + e^{\pmb \omega^T\pmb x+ b})]\\
\end{aligned}
$$

稍微转换一下变成求最小值

$$
L(\pmb\omega;b) = \sum_{i = 1}^m[-y_i(\pmb \omega^T\pmb x+ b) + \ln(1 + e^{\pmb \omega^T\pmb x+ b})]
$$

这个函数是一个凸函数，所以梯度下降和牛顿迭代法都可以最小化这个目标函数。先把参数作整合便于求导

$$
\pmb\beta = (\pmb \omega ; b), \hat{\pmb x} = (\pmb x; 1)
$$

于是

$$
\pmb \omega^T\pmb x+ b = \pmb\beta^T \pmb{\hat x}
$$

然后求导

$$
\dfrac{\partial L(\pmb\beta)}{\partial \pmb\beta} = -\sum_{i = 1}^m\hat{\pmb x}_i(y_i - p(y_i = 1|\hat{\pmb x};\pmb\beta))
$$

$$
\dfrac{\partial^2 L(\pmb\beta)}{\partial\pmb\beta\partial\pmb\beta^T} = \sum_{i = 1}^m\hat{\pmb x_i}\hat{\pmb x_i}^T p(y_i = 1|\hat{\pmb x};\pmb\beta)(1 - p(y_i = 1|\hat{\pmb x};\pmb \beta))
$$

然后就是做梯度下降或者牛顿迭代法来优化这个损失函数了。