# 高斯混合聚类

## 写在前面

聚类既然是无监督学习方法，那么自然会有多种多样的策略来从不同角度解释并分类数据，高斯混合聚类就是其中一种。

高斯混合聚类的核心思想不同于 K 均值聚类的启发式方法，它的核心思想反而与 EM 算法非常类似，本质上是一种经典概率论方法。高斯混合聚类将训练数据视为高斯混合模型（一种概率模型）的观测值，并且以最有可能观测到某个样本的高斯分量作为该样本的簇标记，因此高斯混合模型属于**原型聚类**。这样一来，只需要将训练数据作为观测值，使用 EM 算法求解高斯混合模型的参数即可完成聚类任务。

## 高斯混合模型

假设观测数据 $\pmb y_1,\pmb y_2,\cdots,\pmb y_n\in R^d$ 由高斯混合模型生成

$$
P(Y|\theta) = \sum_{k = 1}^K\alpha_k\phi(Y|\theta_k)
$$

其中 $\alpha_k$ 表示第 $k$ 个高斯分量的系数，$\sum_{k = 1}^K\alpha_k = 1$，$\theta = (\alpha_1,\cdots,\alpha_K;\theta_1,\cdots,\theta_K)$ 是模型参数。$\phi(Y|\theta_k)$ 是高斯分布函数，$\theta_k = (\alpha_k, \pmb\mu_k, \pmb\Sigma_k)$，其中 $\pmb\mu_k$ 是第 $k$ 个高斯分量的均值向量，$\pmb\Sigma_k$ 是协方差矩阵。

多维高斯分布函数长这样

$$
\phi(Y|\theta_k) = \dfrac{1}{(2\pi)^\frac{n}{2}|\pmb\Sigma_k|^\frac{1}{2}}\exp\left(-\dfrac{1}{2}(Y - \pmb\mu_k)^T\pmb\Sigma^{-1}(Y-\pmb\mu_k)\right)
$$


> 注：
> （1）在 EM 算法的推导中，我们并没有关注观测数据所在的空间维度，因为那时我们不必考虑具体的分布函数的表达式。
> （2）均值向量 $\pmb\mu$ 和协方差矩阵 $\Sigma$ 都建立在 $R^d$ 上，设观测值来自 $d$ 维随机变量 $Y = (X_1,X_2,\cdots,X_d)^T$，则
> $$
 \pmb \mu = (E(X_1),E(X_2),\cdots,E(X_d))^T
  $$
> $$
 \pmb\Sigma = [c_{ij}]_{d\times d} = \left[
 \begin{matrix}
 c_{11}& c_{12} &\cdots &c_{1d}\\
 c_{21}& c_{22} &\cdots &c_{2d}\\
 \vdots& \vdots &\ddots &\vdots\\
 c_{d1}& c_{d2} &\cdots &c_{dd}
 \end{matrix}
 \right]
 $$
 > $$
  c_{ij} = Cov(X_i, X_j)
  $$
 
### E步

要使用 EM 算法，首先要构建 $Q$ 函数，即该模型的对数似然函数的期望。在那之前让我们先对该模型进行一些更具体的表达（因为该模型含有隐变量）。

对于任意一个观测值 $y_j$，可以认为它是这样产生的：先根据概率 $\pmb\alpha = (\alpha_1,\cdots,\alpha_K)^T$ 选择一个高斯分量，再根据这个高斯分量的概率分布产生 $\pmb y_j$。当然，可以选择的高斯分量不是唯一的，$\pmb y_j$ 可以从任意一个高斯分量中产生，因此最终的概率是一个和式。

但是既然如此，就意味着我们无法得知一次观测的 $\pmb y_j$ 究竟来自哪一个高斯分量。也就是说，虽然我们知道 $\pmb y_j$ 可以由哪些路径产生，但我们无法通过 $\pmb y_j$ 判断它来自具体哪条路径。那么，该问题就存在隐变量：产生 $\pmb y_j$ 的高斯分量。

我们不妨用 $\gamma_{jk}$ 表示这一信息

$$
\gamma_{jk} = \left\{
\begin{aligned}
&1,& y_j 来自第 k 个分量\\
&0,& 否则
\end{aligned}
\right.
$$
于是高斯混合模型的对数似然函数应为

$$
\begin{aligned}
L(\theta) 
&= \log\left\{\prod_{j = 1}^n P(\pmb y_j|\theta)\right\}\\
&= \log\left\{ \prod_{j = 1}^n \prod_{k = 1}^K[\alpha_k\phi(\pmb y_j|\theta)]^{\gamma_{jk}}\right\}\\
&= \log\left\{ \prod_{k = 1}^K\alpha_k^{n_k}\prod_{j = 1}^n\phi(\pmb y_j|\theta)^{\gamma_{jk}} \right\}\\
&= \sum_{k = 1}^K\left\{n_k\log \alpha_k + \sum_{j = 1}^n\gamma_{jk}\left[\dfrac{n}{2}\log(2\pi) -\dfrac{1}{2}\log |\pmb\Sigma_k| - \dfrac{1}{2}(\pmb y_j - \pmb\mu_k)^T\pmb\Sigma_k(\pmb y_j - \pmb\mu_k)\right]\right\}
\end{aligned}
$$

其中 $n_k = \sum_{j = 1}^n \gamma_{jk}$。

很长，但这还没完（不要忘记 $Q$ 函数的定义），因为我们接下来还要求这玩意的期望 $Q(\theta,\theta^{(i)}) = E(L(\theta))$

$$
\begin{aligned}
Q(\theta, \theta^{(i)}) 
&= E[\log P(Y, \gamma|\theta)|y,\theta^{(i)}]\\
&= \sum_{k = 1}^K\left\{n_k\log \alpha_k + \sum_{j = 1}^n\hat\gamma_{jk}\left[\dfrac{n}{2}\log(2\pi) -\dfrac{1}{2}\log |\pmb\Sigma_k| - \dfrac{1}{2}(\pmb y_j - \pmb\mu_k)^T\pmb\Sigma_k(\pmb y_j - \pmb\mu_k)\right]\right\}
\end{aligned}
$$

看起来跟 $L(\theta)$ 差不多，只有 $\gamma$ 的地方发生了改变，注意

$$
\hat\gamma_{jk} = \dfrac{\alpha_k\phi(\pmb y_j|\theta_k)}{\sum_{k = 1}^K\alpha_k\phi(\pmb y_j|\theta_k)}
$$

至此 $Q$ 函数求得，想要求出 $Q$ 还是需要有扎实的数理基础的。

### M步

完成了 $Q$ 函数的构建，就可以进入 M 步了

$$
\theta^{(i + 1)} = \arg\max_{\theta} Q(\theta,\theta^{(i)})
$$

分别对所有 $\alpha_k, \pmb\mu_k, \pmb\Sigma_k$ 求偏导求极值就行了，然后就是不断迭代直到收敛。

> 注：可以看到 EM 算法最困难的地方在于构建 $Q$ 函数和对其求偏导的过程。