# 支持向量机及其数学基础

## 支持向量机

支持向量机这名字看上去叫人不懂，但是学习过后其实还是比较形象的。所谓支持向量，其实就是用来“支撑”一个最优分类超平面使之约束在固定位置的一系列支点。

支持向量机要解决的问题与感知机、贝叶斯法之类其实没什么区别，都是用来做回归或者分类的，它独特的地方在于引入了支持向量和核技巧，而且由于更多数学上的考虑（试图找到假设空间中最优的分类超平面），比大多数方法效果更好，可解释性也更强。

支持向量机严格地基于最优化理论的方法和手段，通过**使得两类样本有最大的间隔**来确定最优划分超平面。其中，支持向量指那些离划分超平面最近的满足特定条件的点，核技巧则是一种将非线性分类问题映射到高维使其变得线性可分的手段。

### 线性可分支持向量机

线性可分支持向量机用于解决在线性可分的数据集上求解使两个类别间隔最大的分类超平面，具体的说，用于求解一个分类函数 $f(x) = \text{sgn}(\pmb\omega^T\pmb x + \pmb b)$ 的参数 $(\pmb\omega, b)$，使得它表现在数学意义上最好，具体地说，使得所有支持向量到超平面的距离最大。

### 线性支持向量机

线性支持向量机同样被用于求解一个分类函数 $f(x) = \text{sgn}(\pmb\omega^T\pmb x + \pmb b)$，但不同的是它通过引入一个对误分类点的宽容度来兼容近似线性可分的训练数据，也就是说，在超平面附近允许分布一些点，甚至可能是误分类点，而不需要严格分隔两类点。

### 非线性支持向量机

线性支持向量机只能解决线性可分的分类问题，这显然不能满足现实中的大多数情况。核技巧很好地解决了这个问题，他的主要思想源于这样一种观点——低维的非线性问题可能是高维线性问题的投影，因为在同等信息量下，高维空间或许能够用参数复杂性替代样本点的空间复杂性。于是，借助于核函数，核方法将一个低维的非线性问题映射到高维空间（希尔伯特），使其变得线性可分。

## SVM的数学基础

* SVM的本质，是通过求解一个包含分类函数参数的凸二次规划问题来确定最优分类超平面。所以，SVM其实是一个最优化问题（而不是什么玄学的机器学习）的求解器，它不是一种拟合的方法。

### 凸二次规划

凸二次规划是一种约束最优化问题，形如

$$
\begin{aligned}
    &\min_{\pmb x} f(\pmb x) = \dfrac{1}{2}\pmb x^TA\pmb x + B^T\pmb x,\ x\in R^n\\
    st.\ &g_i(\pmb x) \le 0,\quad i = 1, 2, \cdots, k\\
    &h_i(\pmb x) = 0,\quad i = 1, 2, \cdots, l
\end{aligned}
$$

* 其中 $A$ 是一个 $n\times n$ 矩阵，$B$ 是一个 $n$ 维向量。线性代数的知识告诉我们，当 $A$ 正定/半正定时，目标函数 $f(x)$ 是凸函数，存在最小值。
* 此外，$g_i(x)$ 称为约束函数，凸二次规划中它必须是仿射函数，即线性函数。
* 而 $h_i(x)$ 也是约束函数，只是它们造成的约束不同，它们是仿射函数。

在支持向量机问题中，我们的核心目标是最大化一个称为「**间隔**」的变量，用于描述两类样本之间的一种距离，我们在后面将反复提到它，此处先不作详细讨论。

我们只需要知道，**为了化用「凸二次规划」这样一种最优化模型，我们试图将「间隔」这一变量以二次型的形式表达**，并根据训练数据设计一系列约束来将一个线性分类问题转化为凸二次规划问题。如果求出了凸二次规划问题的最优解，我们就能得到最优分类超平面。

### 拉格朗日乘子法

有了分析和表述问题的模型（凸二次规划），我们便要着手考虑解决问题的方法，从而得到问题的答案。最优化理论（其实高数就提过）告诉我们，求解约束最优化问题可以使用[拉格朗日乘子法](../../高等数学（下）/第六章/多元函数极值问题.md#拉格朗日乘数法)，对于上面的凸二次规划问题，可以通过引入拉格朗日乘子写成拉格朗日函数

$$
F(\pmb x, \pmb\alpha, \pmb \beta) = f(x) + \sum_{i = 1}^k \alpha_i g_i(\pmb x) + \sum_{i = 1}^l\beta_ih_i(\pmb x)
$$

来解决该问题，其中任意 $\alpha_i, \beta_i$ 称为拉格朗日乘子，且有 $\alpha_i \ge 0$，求解 $F$ 对所有参数 $\pmb x, \pmb \alpha, \pmb \beta$ 的偏导并令其为 $0$ 即可求得极值处的参数值。

但是实际上我们会看到，SVM 没有采用这种做法，而是利用了**拉格朗日对偶性**来求解，这种方法通过构造原始问题（即嗯求偏导）的一对**极大极小问题和极小极大问题**来间接地给出原始问题的极值点。原因有三：（1）求极值点的过程是解线性方程组，约束条件很多的时候矩阵相当大，求解很慢，利用对偶性可以把大矩阵的一次消元变成小矩阵的几次消元，快一点（其实还是不够快，后面会有所谓 SMO 法进一步优化）（2）利用拉格朗日对偶问题的形式，SVM可以方便地引入核函数来处理非线性分类问题的情况（3）当对偶问题的其中一个比较难解时，另一个可能容易解出。

> 注：这个问题和原始的拉格朗日乘子法有略微的不同，因为它引入了「不等式约束」，这使得问题的可行域变大了许多，原始的拉格朗日乘子法实际上解决不了这个问题（注意到上述拉格朗日函数有条件 $\alpha_i \ge 0$，这是原始方法所不要求的）。但是通过引入 KKT 条件，拉格朗日乘子法兼容了不等式约束（后面我们还将看到满足 KKT 条件的拉格朗日函数具有对偶性），可以根据该条件来求得极值点（**KKT 条件是极值点的充要条件**）。
> 
> KKT 条件：
> 
>$$
\left\{
\begin{aligned}
    &\nabla_x F(\pmb x, \pmb\alpha, \pmb\beta) = 0,\\
    &h_i(\pmb x) = 0,\\
    &\alpha_i \ge 0,\\
    &g_i(\pmb x)\le 0,\\
    &\alpha_ig_i(\pmb x) = 0
\end{aligned}
\right.
$$
> 其中，$\alpha_i \ge 0$ 称为「对偶可行性」，$\alpha_ig_i(\pmb x) = 0$ 称为「互补松弛性」。
> 
> 关于 KKT 条件的来源，可以参考[这篇更易懂的文章](https://zhuanlan.zhihu.com/p/154517678)，以及[这篇更严谨的文章](https://zhuanlan.zhihu.com/p/38163970)，建议结合起来着重看懂其中关于「不等式约束」的部分即可（最重要的部分即 $\alpha_i \ge 0$ 的原因）。
> 
> 关于 KKT 条件的适用范围，维基百科给出了更详细的[解释](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_(or_constraint_qualifications))，这一特性使得我们的 $g_i(x)$ 必须满足特定的性质（如凸二次规划中必须是仿射函数）。

### 拉格朗日对偶性

对偶性是最优化·运筹学的概念，对偶问题通过原始问题经一定的转化得来，与原始问题有一定关系（弱对偶、强对偶，以及他们的条件）。一般来说，对偶问题用来解决凸优化问题更为有效，此外，线性规划问题一定存在对偶问题，凸二次规划同时满足了这两条性质，所以它具有对偶问题。

> 注：设原问题是极小极大问题，它的最优解 $p^*$，对偶问题是极大极小问题，它的最优解 $d^*$，则
> （1）弱对偶 $p^* \ge d^*$，在任何情况下都成立。
> （2）强对偶 $p^* = d^*$，满足某些 $CQ$ (constraint qualifications) 条件才成立，维基百科给出了一份[清单](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_(or_constraint_qualifications))。
> 
> 弱对偶情况的结论实际上是显然的，同一个问题的等价极大极小问题的顶点势必要矮于极小极大问题的谷点。

凸二次规划问题是强对偶的，它满足 $SCQ$ 和 $LCQ$ 条件，这意味着我们可以通过求解原始凸二次规划问题的对偶问题来间接地求得原问题的解。

对于原问题

$$
\begin{aligned}
    &\min_{\pmb x} f(\pmb x) = \dfrac{1}{2}\pmb x^TA\pmb x + B^T\pmb x,\ x\in R^n\\
    st.\ &g_i(\pmb x) \le 0,\quad i = 1, 2, \cdots, k\\
    &h_i(\pmb x) = 0,\quad i = 1, 2, \cdots, l
\end{aligned}
$$

假设它的可行域为 $K$，先将其写成一个拉格朗日函数

$$
F(\pmb x, \pmb\alpha, \pmb \beta) = f(x) + \sum_{i = 1}^k \alpha_i g_i(\pmb x) + \sum_{i = 1}^l\beta_ih_i(\pmb x)
$$

考虑 $\pmb x$ 的一个函数

$$
\theta_P(\pmb x) = \max_{\alpha,\beta:\alpha_i \ge 0} F(\pmb x, \pmb \alpha, \pmb \beta)
$$

有趣的是，当我们考察 $\theta_P(\pmb x)$ 在 $R^n$ 上的取值，将发现一些神奇的性质

$$
\theta_P(\pmb x) = \left\{
\begin{aligned}
    &f(\pmb x)&, \pmb x\in K\\
    &+\infty&,else
\end{aligned}
\right.
$$

这是因为当 $\pmb x\not\in K$ 时，有 $g_i(\pmb x) > 0$，那么取任意大的 $\alpha_i$ 就会使得 $\sum_{i = 1}^k \alpha_i g_i(\pmb x) \rightarrow \infty$，此外，还有 $h_i(\pmb x) \neq 0$，则可以任取 $\beta_i$ 使得 $\sum_{i = 1}^l\beta_ih_i(\pmb x) \rightarrow \infty$，换言之只要约束条件不被满足，一定有 $\underset{\alpha,\beta:\alpha_i\ge 0}{\max} F\rightarrow +\infty$。

而当 $x\in K$ 时，根据 $h_i(\pmb x) = 0, g_i(\pmb x) \le 0, \alpha_i \ge 0$ 显然可得结论。

这是一个令人兴奋的结论，因为我们将 $f(\pmb x)$ 从拉格朗日函数 $F(\pmb x, \pmb\alpha, \pmb\beta)$ 中抽取了出来！只需要再进行一步

$$
\min_{\pmb x} \theta_P(\pmb x)
$$

就能得到与原问题一模一样的极值点！这就是说，我们将原问题转化成了一个等价的极小极大问题，我们不妨假设该问题的解为 $p^*$。

根据最优化的对偶理论，这个问题（$\min_{\pmb x}\theta_P(\pmb x)$）是存在对偶问题的，且它是一个极大极小问题，下面我们来考察这个对偶问题。

$$
\max_{\pmb\alpha, \pmb\beta: \alpha_i \ge 0}\min_{\pmb x} F(\pmb x, \pmb\alpha, \pmb\beta)
$$

因为这是一个凸二次规划问题，上面提到它是强对偶的，如果它的解是 $d^*$，那么有

$$
p^* = d^* = F(\pmb x^*, \pmb\alpha^*, \pmb\beta^*)
$$

其中 $(\pmb x^*, \pmb\alpha^*, \pmb\beta^*)$ 就是该问题对应的最优解，而且是唯一的最优解，因为这两个函数一个是凸函数，一个是凹函数，那么最优解就是唯一的。

要求出具体的解值需要借助 KKT 条件，要利用 KKT 条件求最优解，优化问题首先需要满足三条性质。

（1）$f(\pmb x)$ 和 $g_i(\pmb x)$ 是凸函数。

（2）$h_i(\pmb x)$ 是仿射函数。

（3） $g_i(\pmb x)$ 是严格可行的，即 $\exists \pmb x, st. \forall i, g_i(\pmb x) < 0$，通俗的说就是可行域是有面积的。

对于凸二次规划，（1）（2）显然是符合的（仿射函数也是凸函数），那么我们的问题只需要满足（3）就可以被很好地解决（即便不满足，也能够被近似地解决）。

> 注意：实际上，在一般的机器学习问题中，如SVM，并不会直接使用数学方法根据 KKT 条件解出最优解，而是采取步进的优化策略，寻求可能的最优解以降低算法复杂度或不可解问题的近似解。