# 硬间隔最大化SVM

## 问题引入

我们将用 SVM 解决的问题，与我们在感知机中看到的是一致的——给定一组输入空间上的点集 $\{(\pmb x_i, y_i)\}_{i = 1}^n, \pmb x_i\in R^n, y_i\in \{+1, -1\}$，希望找到一个分类超平面 $f(x) = \text{sgn}(\pmb\omega^T\pmb x + b)$，将 $y = +1$ 的点和 $y = -1$ 的点正确分开。

让我们回忆一下感知机的解决思路，它通过构造一个函数来表达所有误分类点到分类超平面的几何距离，并最小化这个距离来寻找最好的超平面参数。这种方法有一个缺陷，即它找到的分类超平面可能泛化能力比较差（可能离某类点更近，因为这类点中误分类点较多），此外，由它为基本模型构建的神经网络的可解释性也不是那么好。

SVM 致力于解决这个问题，它实质上是将上述问题归结到一个凸优化问题，并借助最优化理论的解决方法来解决问题，因此它具有良好的数学结构，同时由于求解过程的清晰性，它具备相当好的可解释性。

我们在[SVM及其数学基础](./SVM及其数学基础.md)中也提到过，SVM其实就是在解决一个凸优化问题，因此我们核心的解决思路其实相当明晰：

（1）将上述分类问题转换成一个凸优化问题。
（2）使用凸优化的求解方法求解该凸优化问题。
（3）寻找更好的算法以降低求解复杂度。

## 问题转化

我们首先来考虑一个最优的分类超平面应该具有什么样性质，显然，它首先要尽可能地将所有点正确分类，我们需要找到一个合适的标准来进行评判。现在，我们不妨从最简单的严格线性可分的分类问题入手来考虑这个问题。

严格线性可分的分类问题中，我们一定可以找到一些分类超平面完全把两类点分开，即把它们正确的分类。所以我们现在的任务是进一步考虑，要选择其中泛化性能最优秀的作为我们的答案。在这个问题中，不难发现根据仅有的信息，**最优的超平面一定是恰好在两类点的正中间的**，这也就是 SVM 给出的评判标准。

SVM 为了描述一个恰好在两类点正中间的分类超平面，首先引入了两个概念——函数间隔和几何间隔，通过最大化几何间隔来达到目的，我们先对它们进行适当的考察。

### 函数间隔

称

$$
\hat \gamma_i = y_i(\pmb\omega^T\pmb x_i + b)
$$

为函数间隔，由于 $y_i$ 可以表达样本是否分类正确的信息，而 $|\pmb\omega^T\pmb x_i + b|$ 则可以表达样本与分类超平面的相对距离，所以 $y_i(\pmb\omega^T\pmb x_i + b)$ 可以表达某种**分类确信度**的信息，该值越大越有可能分类正确。

但是，该间隔并不是一个绝对值，而是一个相对值。具体的说，当我们等比例缩放 $\pmb \omega$ 和 $b$ 时，虽然分类超平面的位置没有改变，但是函数间隔的值却发生了改变。

### 几何间隔

几何间隔是函数间隔的延伸定义，让我们回忆点到直线的距离公式

$$
d_i = \dfrac{|\pmb\omega^T\pmb x + b|}{||\pmb\omega||}
$$

其实，几何间隔就几乎等同于样本点到分类超平面的距离

$$
\gamma_i = y_i\dfrac{\pmb\pmb\omega^T\pmb x + b}{||\pmb\omega||}
$$

唯一的不同在于，当样本点被误分类时，这个距离将成为负数。

不难发现，$\pmb\pmb\omega, b$ 的同比缩放不会对几何间隔造成影响，所以任意样本点关于超平面计算出的距离（在参数发生改变时）就是有可比性的，也就是可优化的。

### 原始问题

现在，有了函数间隔和几何间隔这两个描述样本点与超平面位置关系的工具，我们便可以来尝试将「最大化间隔」这一问题归结为一个凸优化问题了。

首先来考虑我们的问题，若希望让分类超平面尽可能处于两类样本中间，其实就等价于尽可能让超平面处于两类点的边界的正中间，换言之，等价于边界点到超平面的距离尽可能大（经过良好定义的边界点，就是支持向量）。

于是我们可以先考虑边界点的几何间隔，显然为

$$
\gamma = \min_{1\le i \le n} \gamma_i
$$

所以就有最优化问题

$$
\begin{aligned}
    &\max_{\omega, b}\gamma\\
    &st. y_i\dfrac{\pmb\omega^T\pmb x + b}{||\pmb\omega||} \ge \gamma
\end{aligned}
$$

但是现在这个问题完全没法做，因为我们无法确定目标函数的表达式，于是我们做一些变换（其中 $\hat\gamma$ 为函数间隔，定义与 $\gamma$ 类似，是所有样本点中的最小值）

用关系 $\gamma = \dfrac{\hat\gamma}{||\pmb\omega||}$ 进行代换

$$
\begin{aligned}
    &\max_{\omega, b}\dfrac{\hat\gamma}{||\pmb\omega||}\\
    &st. y_i\dfrac{\pmb\omega^T\pmb x + b}{||\pmb\omega||} \ge \dfrac{\hat\gamma}{||\pmb\omega||}
\end{aligned}
$$


也就是

$$
\begin{aligned}
    &\max_{\omega, b}\dfrac{\hat\gamma}{||\pmb\omega||}\\
    &st. y_i(\pmb\omega^T\pmb x + b) \ge \hat\gamma
\end{aligned}
$$

注意到该问题已经与 $\hat\gamma$ 的取值无关，因此不妨设 $\hat\gamma = 1$，得到

$$
\begin{aligned}
    &\max_{\omega, b}\dfrac{1}{||\pmb\omega||}\\
    &st. y_i(\pmb\omega^T\pmb x + b) \ge 1
\end{aligned}
$$

到这里，我们已经获得了阶段性的成功——我们将间隔从问题中消去了！这意味着整个问题现在不需要考虑从所有间隔中求出最小的一个，而只需要保证 $\pmb\omega$ 的取值，使得离超平面最近的样本点到超平面的距离为 $\dfrac{1}{||\pmb\omega||}$ 就可以了。换言之，我们只需要求解一个只与 $\pmb\omega$ 有关的函数的约束最优化问题了，这个目标函数比之前的容易刻画地多。

但是现在这个问题仍然不是凸优化问题，因此我们需要利用一些简单的技巧继续变换这个问题。

容易发现最大化 $\dfrac{1}{||\omega||}$ 实际上与最小化 $\dfrac{1}{2}||\omega||^2$ 等价，于是

$$
\begin{aligned}
    &\max_{\omega, b}\dfrac{1}{2}||\omega||^2\\
    &st. y_i(\pmb\omega^T\pmb x + b) \ge 1
\end{aligned}
$$

问题变为一个凸优化问题，而且是一个凸二次规划问题，只要求出该问题的解，就能得到最优超平面。

> 注意：
> 
> 问题的解只与样本点中离超平面最近的那些点有关，严格地说，只与那些满足 $y_i(\pmb\omega^T\pmb x + b) = 1$ 的点有关（它们到超平面的距离严格地等于 $\dfrac{1}{||\omega||}$）。因为稍微改变这些点将导致问题的解发生移动，而改变其它点则不会造成影响（除非它成为新的最近点）。我们把这些点定义为「支持向量」，这相当形象，很好地给出了支持向量机的特点——那些与分类超平面间隔最小的点支撑着这个超平面。

## 解决问题

将问题转化成凸二次规划问题后，我们就可以利用之前提到的拉格朗日乘子法和拉格朗日对偶性来解决这个最优化问题。具体地说，先构造拉格朗日函数

$$
F(\pmb\omega, b, \pmb\alpha) = \dfrac{1}{||\omega||^2} - \sum_{i = 1}^n \alpha_iy_i(\pmb\omega^T\pmb x + b) + \sum_{i = 1}^n\alpha_i
$$

其中 $\alpha_i \ge 0$（KKT条件），$\pmb\alpha = (\alpha_1,\cdots,\alpha_n)^T$ 是拉格朗日乘子向量。