# 硬间隔最大化SVM

## 问题引入

我们将用 SVM 解决的问题，与我们在感知机中看到的是一致的——给定一组输入空间上的**线性可分**的点集 $\{(\pmb x_i, y_i)\}_{i = 1}^n, \pmb x_i\in R^d, y_i\in \{+1, -1\}$，希望找到一个分类超平面 $f(x) = \text{sgn}(\pmb\omega^T\pmb x + b)$，将 $y = +1$ 的点和 $y = -1$ 的点正确分开。

![Screenshot-from-2022-12-14-22-00-59.png](http://image.tjzfile.xyz/images/2022/12/14/Screenshot-from-2022-12-14-22-00-59.png)

让我们回忆一下感知机的解决思路，它通过构造一个函数来表达所有误分类点到分类超平面的几何距离，并最小化这个距离来寻找最好的超平面参数。这种方法有一个缺陷，即它找到的分类超平面可能泛化能力比较差（可能离某类点更近，因为这类点中误分类点较多），此外，由它为基本模型构建的神经网络的可解释性也不是那么好。

SVM 致力于解决这个问题，它实质上是将上述问题归结到一个凸优化问题，并借助最优化理论的解决方法来解决问题，因此它具有良好的数学结构，同时由于求解过程的清晰性，它具备相当好的可解释性。

我们在[SVM及其数学基础](./SVM及其数学基础.md)中也提到过，SVM其实就是在解决一个凸优化问题，因此我们核心的解决思路其实相当明晰：

（1）将上述分类问题转换成一个凸优化问题。
（2）使用凸优化的求解方法求解该凸优化问题。
（3）寻找更好的算法以降低求解复杂度。

## 问题转化

我们首先来考虑一个最优的分类超平面应该具有什么样性质，显然，它首先要尽可能地将所有点正确分类，我们需要找到一个合适的标准来进行评判。现在，我们不妨从最简单的严格线性可分的分类问题入手来考虑这个问题。

严格线性可分的分类问题中，我们一定可以找到一些分类超平面完全把两类点分开，即把它们正确的分类。所以我们现在的任务是进一步考虑，要**选择其中泛化性能最优秀的作**为我们的答案。在这个问题中，不难发现根据仅有的信息，**最优的超平面一定是恰好在两类点的正中间的**，这也就是 SVM 给出的评判标准。

SVM 为了描述一个恰好在两类点正中间的分类超平面，首先引入了两个概念——函数间隔和几何间隔，通过最大化几何间隔来达到目的，我们先对它们进行适当的考察。

### 函数间隔

称

$$
\hat \gamma_i = y_i(\pmb\omega^T\pmb x_i + b)
$$

为函数间隔，由于 $y_i$ 可以表达样本是否分类正确的信息，而 $|\pmb\omega^T\pmb x_i + b|$ 则可以表达样本与分类超平面的相对距离，所以 $y_i(\pmb\omega^T\pmb x_i + b)$ 可以表达某种**分类确信度**的信息，该值越大越有可能分类正确。

但是，该间隔并不是一个绝对值，而是一个相对值。具体的说，当我们等比例缩放 $\pmb \omega$ 和 $b$ 时，虽然分类超平面的位置没有改变，但是函数间隔的值却发生了改变。

### 几何间隔

几何间隔是函数间隔的延伸定义，让我们回忆点到直线的距离公式

$$
d_i = \dfrac{|\pmb\omega^T\pmb x_i + b|}{||\pmb\omega||}
$$

其实，几何间隔就几乎等同于样本点到分类超平面的距离

$$
\gamma_i = y_i\dfrac{\pmb\omega^T\pmb x_i + b}{||\pmb\omega||}
$$

唯一的不同在于，当样本点被误分类时，这个距离将成为负数。

不难发现，$\pmb\omega, b$ 的同比缩放不会对几何间隔造成影响，所以任意样本点关于超平面计算出的距离（在参数发生改变时）就是有可比性的，也就是可优化的。

### 原始问题

现在，有了函数间隔和几何间隔这两个描述样本点与超平面位置关系的工具，我们便可以来尝试将「最大化间隔」这一问题归结为一个凸优化问题了。

首先来考虑我们的问题，若希望让分类超平面尽可能处于两类样本中间，其实就等价于尽可能让超平面处于两类点的边界的正中间，换言之，等价于边界点到超平面的距离尽可能大（经过良好定义的边界点，就是支持向量）。

![Screenshot-from-2022-12-14-22-04-46.png](http://image.tjzfile.xyz/images/2022/12/14/Screenshot-from-2022-12-14-22-04-46.png)

于是我们可以先考虑边界点的几何间隔，显然为

$$
\gamma = \min_{1\le i \le n} \gamma_i
$$

所以就有最优化问题

$$
\begin{aligned}
    &\max_{\omega, b}\gamma\\
    &st. y_i\dfrac{\pmb\omega^T\pmb x_i + b}{||\pmb\omega||} \ge \gamma
\end{aligned}
$$

但是现在这个问题完全没法做，因为我们无法确定目标函数的表达式，于是我们做一些变换（其中 $\hat\gamma$ 为函数间隔，定义与 $\gamma$ 类似，是所有样本点中的最小值）

用关系 $\gamma = \dfrac{\hat\gamma}{||\pmb\omega||}$ 进行代换

$$
\begin{aligned}
    &\max_{\omega, b}\dfrac{\hat\gamma}{||\pmb\omega||}\\
    &st. y_i\dfrac{\pmb\omega^T\pmb x_i + b}{||\pmb\omega||} \ge \dfrac{\hat\gamma}{||\pmb\omega||}
\end{aligned}
$$


也就是

$$
\begin{aligned}
    &\max_{\omega, b}\dfrac{\hat\gamma}{||\pmb\omega||}\\
    &st. y_i(\pmb\omega^T\pmb x_i + b) \ge \hat\gamma
\end{aligned}
$$

注意到该问题已经与 $\hat\gamma$ 的取值无关，因此不妨设 $\hat\gamma = 1$，得到

$$
\begin{aligned}
    &\max_{\omega, b}\dfrac{1}{||\pmb\omega||}\\
    &st. y_i(\pmb\omega^T\pmb x_i + b) \ge 1
\end{aligned}
$$

到这里，我们已经获得了阶段性的成功——我们将间隔从问题中消去了！这意味着整个问题现在不需要考虑从所有间隔中求出最小的一个，而只需要保证 $\pmb\omega$ 的取值，使得离超平面最近的样本点到超平面的距离为 $\dfrac{1}{||\pmb\omega||}$ 就可以了。换言之，我们只需要求解一个只与 $\pmb\omega$ 有关的函数的约束最优化问题了，这个目标函数比之前的容易刻画地多。

![Screenshot-from-2022-12-14-22-07-22.png](http://image.tjzfile.xyz/images/2022/12/14/Screenshot-from-2022-12-14-22-07-22.png)

但是现在这个问题仍然不是凸优化问题，因此我们需要利用一些简单的技巧继续变换这个问题。

容易发现最大化 $\dfrac{1}{||\omega||}$ 实际上与最小化 $\dfrac{1}{2}||\omega||^2$ 等价，于是

$$
\begin{aligned}
    &\min_{\omega, b}\dfrac{1}{2}||\omega||^2\\
    &st. y_i(\pmb\omega^T\pmb x_i + b) \ge 1
\end{aligned}
$$

问题变为一个凸优化问题，而且是一个凸二次规划问题，只要求出该问题的解，就能得到最优超平面。

> 注意：
> 
> 问题的解只与样本点中离超平面最近的那些点有关，严格地说，只与那些满足 $y_i(\pmb\omega^T\pmb x + b) = 1$ 的点有关（它们到超平面的距离严格地等于 $\dfrac{1}{||\omega||}$）。因为稍微改变这些点将导致问题的解发生移动，而改变其它点则不会造成影响（除非它成为新的最近点）。我们把这些点定义为「支持向量」，这相当形象，很好地给出了支持向量机的特点——那些与分类超平面间隔最小的点支撑着这个超平面。

## 解决问题

将问题转化成凸二次规划问题后，我们就可以利用之前提到的拉格朗日乘子法和拉格朗日对偶性来解决这个最优化问题。具体地说，先构造拉格朗日函数

$$
F(\pmb\omega, b, \pmb\alpha) = \dfrac{1}{2}{||\pmb\omega||^2} - \sum_{i = 1}^n \alpha_iy_i(\pmb\omega^T\pmb x_i + b) + \sum_{i = 1}^n\alpha_i
$$

其中 $\alpha_i \ge 0$（KKT条件），$\pmb\alpha = (\alpha_1,\cdots,\alpha_n)^T$ 是拉格朗日乘子向量。

利用前面提到的拉格朗日对偶性，我们写出一对拉格朗日对偶问题

$$
\begin{aligned}
    &\max_{\pmb\omega, b}\min_{\pmb\alpha} F(\pmb\omega, b,\pmb\alpha)\\
    &st. \alpha_i \ge 0
\end{aligned}
$$

和

$$
\begin{aligned}
    &\min_{\pmb\alpha}\max_{\pmb\omega, b} F(\pmb\omega, b,\pmb\alpha)\\
    &st. \alpha_i \ge 0
\end{aligned}
$$

因为原始问题是凸二次规划问题，因此这对问题具有相同的解，经过实践（俺就不写了）证明，后者的解法将比前者更方便简单。

接下来考虑后者，即上面那个极大极小问题，首先对 $\pmb\omega, b$ 求导

$$
\begin{aligned}
    \dfrac{\partial F}{\partial \pmb\omega} 
    &= \pmb\omega - \sum_{i = 1}^n\alpha_iy_i\pmb x_i = 0
\end{aligned}
\Rightarrow \pmb\omega = \sum_{i = 1}^n\alpha_iy_i\pmb x_i
$$

$$
\dfrac{\partial F}{\partial b} = -\sum_{i = 1}^n\alpha_iy_i = 0
$$

于是乎我们求得了内层极大问题的极值点，遂代入优化问题得

$$
\begin{aligned}
    F(\pmb\omega, b, \pmb x) 
    &= \dfrac{1}{2}(\sum_{i = 1}^n\alpha_iy_i\pmb x_i)^2 - \sum_{i = 1}^n\alpha_iy_i(\pmb\omega^T\pmb x_i + b) + \sum_{i = 1}^n\alpha_i\\
    &= \dfrac{1}{2}\sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_jy_iy_j\pmb x_i^T\pmb x_j - \sum_{i = 1}^n\alpha_iy_i(\sum_{j = 1}^n\alpha_jy_j\pmb x_j^T x_i + b) + \sum_{i = 1}^n\alpha_i\\
    &= -\dfrac{1}{2}\sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_jy_iy_j\pmb x_i^T\pmb x_j + \sum_{i = 1}^n\alpha_i
\end{aligned}
$$

问题转化为

$$
\begin{aligned}
    &\min_{\pmb\alpha}\dfrac{1}{2}\sum_{i = 1}^n\sum_{j = 1}^n\alpha_i\alpha_jy_iy_j\pmb x_i^T\pmb x_j + \sum_{i = 1}^n\alpha_i\\
    st.&\ \forall\alpha_i \ge 0\\
    & \sum_{i = 1}^n\alpha_iy_i = 0
\end{aligned}
$$

有趣的是，这个问题仍然是一个凸二次规划问题，我们便可以尝试直接通过对 $\pmb\alpha$ 求出它的解 $\alpha^*$（这其实就是解线性方程组，只是求导麻烦些），进一步回代（还需要利用 KKT 条件来求，如果忘记可以再看一下[上一节](./SVM及其数学基础.md)内容）就能得到 $\pmb\omega^*, b^*$ 了，而这正是最优超平面的参数。下面假设我们已经求得 $\alpha^*$，来回代求解 $\pmb\omega^*, b^*$。

首先显然有

$$
\pmb\omega^* = \sum_{i = 1}^n\alpha_i^*y_ix_i
$$

它比较好求，但是 $b$ 和 $\alpha^*$ 之间没有直接关系，这该如何是好？我们需要重新审视这个最优化问题到此位置满足的条件和等式，看看还有什么有用的信息。

$$
\sum_{i = 1}^n\alpha_iy_i\pmb x_i = \pmb\omega\\
\sum_{i = 1}^n\alpha_iy_i = 0\\
\alpha_i \ge 0\\
y_i(\pmb\omega^T\pmb x_i + b) \ge 1
$$

对第三个条件进行变换

$$
y_i(\pmb\omega^T\pmb x_i + b) \ge 1 \overset{y_i^2 = 1}{\Rightarrow} \pmb\omega^T\pmb x_i + b \ge y_i
$$

观察这个式子，会发现一个有趣的事实，即 $b$ 事实上与样本点的关系不是那么紧密，或者说任意的样本点都将满足上述事实。那么我们是否可以考虑通过一些特殊情况使得上述不等式取等号，进而得到答案？答案是肯定的，这些特殊样本点实际上就是支持向量，它们使得 $\pmb\omega^T\pmb x_i + b = y_i$。

这样一来，我们有

$$
b^* = \hat y_i - \sum_{j = 1}^na_jy_j\pmb x_j^T\pmb x_i
$$

问题到此得到充分的解答。

但是不难发现这里有一个问题——我们需要对所有 $\alpha_i$ 求导，而且求导后问题还是相当复杂，我们是否有其他方法呢？答案是肯定的，我们将在后续内容中看到一种步进求解算法——序列最小最优化（SMO）算法，它每次只选取两个样本点作为依据利用拉格朗日函数的梯度信息进行更新，直到更新幅度小于某个阈值。