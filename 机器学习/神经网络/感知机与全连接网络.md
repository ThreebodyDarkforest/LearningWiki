# 感知机、神经元到全连接网络

## 写在前面

感知机是神经网络和支持向量机的基础，它是一个线性分类器，可以作为神经网络的一个最简单神经元（后续还有更复杂的神经元形式），也可以作为支持向量机的基础模型。

全连接网络是最简单的神经网络，是神经网络的基本范式，其它复杂的神经网络基本都是基于全连接网络发展而来的。

## 感知机

感知机是一个二分类线性分类模型，输入为样本的特征向量，输出为样本的类别，方法是在空间中找到一个将二类分离的超平面。为了寻求最优超平面的参数，仍然使用梯度下降法最小化关于误分类信息的损失函数。

### 感知机结构和定义

假设特征空间为 $\mathcal{X}\subset R^n$，输出空间 $\mathcal{Y} = \{1, -1\}$，样本特征向量 $\pmb{x}\in \mathcal X$，输出 $y\in \mathcal Y$ 表示样本类别。假设空间的一个函数

$$
f(x) = \text{sign}(\pmb\omega \cdot \pmb x + b)
$$

称为感知机。其中 $\pmb\omega（权值）, b$ （偏置）为模型参数，$\text{sign}$ 是符号函数。

![Screenshot-from-2022-11-11-04-37-32.png](http://image.tjzfile.xyz/images/2022/11/11/Screenshot-from-2022-11-11-04-37-32.png)

有了分类模型，接下来就要考虑如何进行模型学习了。首先必须考虑模型学习需要优化的损失函数，它必须易求且可微，然后就是考虑优化它的方法。

> 注：
> （1）线性可分：如果数据集中的正例和反例可以被一个超平面完全分离，那么这个数据集就是线性可分的。

### 感知机学习策略

由于我们必须找到一个关于参数 $\pmb\omega, b$ 可微的损失函数，所以将它们都用上是最好的方案，显然超平面在空间中的位置特性是与参数强相关的，故应首先考虑位置关系，那么距离是一个好的选择。

根据点到直线的距离公式，容易推得任意点到超平面的距离

$$
l(\pmb x_0) = \dfrac{|\pmb\omega\cdot\pmb x_0 + b|}{||\pmb\omega||}
$$

而且，对于误分类的数据 $(\pmb x_i,y_i)$，分类器所得类别与真实类别相反，即必然有

$$
-y_i(\pmb\omega\cdot\pmb x_i + b) > 0
$$

更进一步，由于任意 $|y_i| = 1$，所以误分类点到超平面的距离为

$$
-\dfrac{1}{||\pmb\omega||}y_i(\pmb\omega\cdot\pmb x_i + b)
$$

则所有误分类点到超平面的总距离

$$
-\dfrac{1}{||\pmb\omega||}\sum_{\pmb x_i\in M}y_i(\pmb\omega\cdot \pmb x_i + b)
$$

其中 $M$ 为所有误分类点的集合，它对于所有误分类点可导。

不考虑 $\dfrac{1}{||\pmb\omega||}$，取

$$
L(\pmb\omega;b) = -\sum_{\pmb x_i\in M}y_i(\pmb\omega\cdot \pmb x_i + b)
$$

作为感知机的损失函数。

### 感知机学习方法

由于批量梯度下降可能存在样本对损失的贡献相互抵消从而导致模型无法很好收敛的问题，感知机学习方法采用随机梯度下降，每轮只随机取一个样本进行梯度下降。

假设随机选取的误分类点为 $(x_i, y_i)$，直接求导可得

$$
\dfrac{\partial L}{\partial\pmb\omega} = -x_iy_i
$$

$$
\dfrac{\partial L}{\partial b} = -y_i
$$

那么每次参数更新为

$$
\pmb\omega \leftarrow \pmb\omega + \eta \pmb x_iy_i
$$

$$
b \leftarrow b + \eta y_i
$$

> 注：
>（1）可以期待在线性可分的数据集上，梯度下降一定会在有限步后收敛，得到一个分离超平面（Novikoff定理）。
>（2）感知机学习算法存在对偶形式，即将训练集各点误分类次数作为参数进行训练。

## 神经元模型

* 典型的神经元是MP神经元，它被用于全连接神经网络中，它通过一个仿射函数接收输入信号，经过一个激活函数变换后进行输出。

![R-C.jpg](http://image.tjzfile.xyz/images/2022/11/14/R-C.jpg)

神经元模型是感知机的推广，它可以被如下形式化描述。

假设特征空间为 $\mathcal{X}\subset R^n$，输出空间 $\mathcal{Y} = R^d$，样本特征向量 $\pmb{x}\in \mathcal X$，输出 $y\in \mathcal Y$ 表示样本类别。假设空间的一个函数

$$
f(x) = a(\pmb\omega \cdot \pmb x + b)
$$

称为感知机。其中 $\pmb\omega（权值）, b$ （偏置）为模型参数，$a$ 是激活函数。

其中

$$
z = \pmb\omega \cdot \pmb x + b = \sum_{i = 1}^n\omega_i x_i + b
$$

又称为仿射函数。注意到，一个神经元实际上就是**两个函数的复合的抽象模型**，这点很重要，是反向传播的数学基础。

> 注：
>（1）神经元不止MP神经元一种，它有相当多的种类，有各自不同的特殊功能和性质。
>（2）一个典型的激活函数是 Sigmoid 函数，它的表达式为 $f(z) = \dfrac{1}{1 + e^{-z}}$，将实数轴上的输入映射到 $(0, 1)$。

## 神经网络

* 神经网络就是用神经元组成的网络，超过两层的网络就被称为深度神经网络了。
* 神经网络在数学上看其实就是一个从特征空间映射到输出空间的非线性函数，这跟其他机器学习算法是一致的。
* 神经网络对给定输入给出一个输出，我们的目标是使得这个输出的概率分布与真实情况尽可能相近，因此为达成这个目标而进行的学习过程，实际上是一个最优化过程。

实际上神经网络有很多种，比如卷积神经网络和LSTM什么的，但是由于本质都是复合函数，所以在学习算法上是可以一致的。这里主要讲最简单的一种——全连接神经网络，或前馈神经网络，之所以说这种网络简单，是因为**它的结构相当规整，内部各子结构一致**。

> 注：有趣的是，前馈神经网络有一个万能逼近定理，这意味着它可以拟合逼近任意函数。要知道某种程度上一个人也可以看成函数，毕竟也可以简化成输入输出。

### 前馈神经网络

前馈神经网络是一种分层结构，每一层具有一些神经元，层内的神经元不互相连接，层与层的神经元才相互连接且全连接，但神经元的**响应在层间是单向传递的**，不会反过来激活，所以叫前馈神经网络。这跟人脑神经元连接结构就不太一样，在人脑中反馈激活/抑制的情况是普遍存在的。总之，其实不管哪种神经元，目前来看都只是对人脑神经元的拙劣模仿。

#### 二层前馈神经网络

* 先从最简单的神经网络说起，有助于理解神经网络的数学本质。

![webwxgetmsgimg.jpg](http://image.tjzfile.xyz/images/2022/11/14/webwxgetmsgimg.jpg)

可以看到，每个输出 $y_l$ 其实就是两层神经元的复合，它可以被严格地表示如下。

输入 $x_i, i\in[1, n]$，输出 $y_k, k\in[1, l]$，如图网络有两层（不包含输入层），第一层（隐藏层）有 $m$ 个神经元，其中第 $j$ 个的输出 $h_j^{(1)}$ 是

$$
h^{(1)}_j = f^{(1)}(\sum_{i = 1}^n\omega_{ji}x_i + b_j^{(1)}), j = 1, 2,\cdots, m
$$

这个神经元的输入就是 $\sum_{i = 1}^n\omega_{ji}x_i + b_j^{(1)}$，包括了输入层所有神经元的输出 $\{x_i\}_{i = 1}^n$ 和一个偏置 $b_j^{(1)}$，$f^{(1)}$ 是该层激活函数。
第二层（输出层）由 $l$ 个神经元组成，其中第 $k$ 个神经元的输出 $y_k$ 是

$$
y_k = f^{(2)}(\sum_{j = 1}^m\omega_{kj}h_j^{(1)} + b_k^{(2)}), k = 1,2,\cdots,l
$$

这个神经元的输入就是 $\sum_{j = 1}^n\omega_{kj}h_j^{(1)} + b_k^{(2)}$，包括了隐藏层所有神经元的输出 $\{h_j^{(1)}\}_{j = 1}^m$ 和一个偏置 $b_k^{(2)}$，$f^{(2)}$ 是该层激活函数。

注意到 $h_j^{(1)}$ 是第一层的输出，可见 $y_k$ 其实是四个函数的复合（两个仿射、两个激活函数）。

神经网络可以用矩阵表示，这使得它更易于计算，也更符合数学逻辑。

设特征向量 

$$
\hat{\pmb x} =
\left[ 
\begin{matrix}
    x_1\\
    x_2\\
    \vdots\\
    x_n\\
    1
\end{matrix}
\right]
$$

隐层输出

$$
\hat{\pmb h}^{(1)} =
\left[ 
\begin{matrix}
    h_1\\
    h_2\\
    \vdots\\
    h_m\\
    1
\end{matrix}
\right]
$$

输出层输出

$$
\pmb y =
\left[ 
\begin{matrix}
    y_1\\
    y_2\\
    \vdots\\
    y_l\\
\end{matrix}
\right]
$$

隐层权重

$$
\pmb W^{(1)} = 
\left[
    \begin{matrix}
        \omega_{11}^{(1)} &\omega_{12}^{(1)} & \cdots &\omega_{1m}^{(1)}\\
        \omega_{21}^{(1)} &\omega_{22}^{(1)} & \cdots &\omega_{2m}^{(1)} \\
        \vdots & \vdots &\ddots &\vdots \\
        \omega_{n1}^{(1)} &\omega_{n2}^{(1)} & \cdots &\omega_{nm}^{(1)} \\
        b_1^{(1)} & b_2^{(1)} & \cdots & b_m^{(1)}
    \end{matrix}
\right]
$$

输出层权重

$$
\pmb W^{(2)} = 
\left[
    \begin{matrix}
        \omega_{11}^{(2)} &\omega_{12}^{(2)} & \cdots &\omega_{1l}^{(2)} \\
        \omega_{21}^{(2)} &\omega_{22}^{(2)} & \cdots &\omega_{2l}^{(2)} \\
        \vdots & \vdots &\ddots &\vdots \\
        \omega_{m1}^{(2)} &\omega_{m2}^{(2)} & \cdots &\omega_{ml}^{(2)} \\
        b_1^{(1)} & b_2^{(1)} & \cdots & b_l^{(1)}
    \end{matrix}
\right]
$$

这样就有

$$
\hat{\pmb h} = f^{(1)}(\pmb W^{(1)^T}\hat{\pmb x})\\
\pmb y = f^{(2)}(\pmb W^{(2)^T} \hat{\pmb h}^{(1)})
$$

> 注：也可以将多个特征列向量组织成矩阵输入，相当于多个样本批量输入网络。

#### 多层前馈神经网络

多层前馈神经网络就是二层网络的简单堆叠，所以其实没什么好说的。

前馈神经网络一般形式如下。

网络输入 $x_i, i=1,2,\cdots,n$ 输出 $y_k, k = 1,2,\cdots,l$，设神经网络有有限的 $s$ 层（不计输入层），假设其中第 $t$ 层由 $m^{(t)}$ 个神经元组成，则第 $t$ 层的第 $j$ 个神经元的输出为

$$
\pmb h_j^{(t)} = f^{(t)}(\sum_{i = 1}^{m^{(t)}}\omega_{ji}h_{i}^{(t - 1)} + b_j^{(t)}), j = 1,2,\cdots,{m^{(t)}}
$$

前馈神经网络也可以写成矩阵的形式，这实际上也是实现神经网络的具体方法。

$$
\left\{
    \begin{aligned}
        &\pmb h^{(1)} = f^{(1)}(\pmb W^{(1)^T}\hat{\pmb x})\\
        &\pmb h^{(2)} = f^{(2)}(\pmb W^{(2)^T}\hat{\pmb h}^{(1)})\\
        &\vdots\\
        &\pmb h^{(s - 1)} = f^{(s - 1)}(\pmb W^{(s - 1)^T}\hat{\pmb h}^{(s - 2)})\\
        &\pmb y = \pmb h^{(s)} = f^{(s)}(\pmb W^{(s)^T}\hat{\pmb h}^{(s - 1)})\\
    \end{aligned}
\right.
$$

> 注：从这些式子可以直观地看出神经网络的数学本质就是多次矩阵乘法和非线性变换的复合。

### 学习策略

* 有了机器学习模型，接下来就是寻求学习策略和学习算法。

跟之前一样，我们也可以用一个损失函数度量模型输出与样本真实标记之间的误差，并力图最小化这个误差来使得模型接近假设空间中真模型，使得模型输出接近样本的真实概率分布。

常用的损失函数有平方损失（回归）、经验损失（分类、回归）、交叉熵损失（分类），根据需要选择即可。

学习策略很好确定，但是学习算法就比较困难了，可以看到，深度神经网络的参数量通常相当多 （$\pmb W$ 矩阵）。对于神经网络学习这种非凸优化问题，基本上就只有梯度下降和牛顿迭代法可以用了。那么问题来了，怎么计算神经网络中每个参数的梯度呢？

其实也不是特别难，前面提到神经网络本质是一个复合函数，所以对它的任意参数应用**链式法则**求导就可以求得参数梯度。因为这一求导过程就像把误差梯度一层一层拨开从输出层反向传递到输出层，所以就直接叫反向传播/误差逆传播了。

> 注：其实这个方法虽然显然而且早已被提出，但是一直未被采用，很大程度上是由于以前没有现在这么充足的算力——你总不可能让大伙手算吧，这可不比手算导弹轨迹简单。更何况，在小规模问题下，神经网络一般不如传统机器学习方法。

### 误差逆传播算法

关于如何求解任意参数梯度的问题，我们不妨从两个方向入手考虑。

![webwxgetmsgimg.jpg](http://image.tjzfile.xyz/images/2022/11/14/webwxgetmsgimg.jpg)

第一种想法也是最自然的想法，就是用链式法则直接做，这就要求我们把损失函数表示成与要求参数 $\omega$ 的嵌套函数，然后根据[多元复合函数求导法则](../../高等数学（下）/第六章/复合函数与隐函数微分.md)求导，不难发现这个求导式子展开之后极其复杂，所以一个一个参数求导显然过于浪费算力。

第二种想法与第一种有些微不同，观察到求导时前层的参数必然用到后层参数梯度，因此它先计算最后一层的所有参数的梯度，然后在计算前层时**复用这一层的梯度**，依次类推，就像一个误差传播的过程，这样计算复杂度就小很多。

比如上面这个图，要计算第一层的参数梯度，可以先把第二层的参数梯度全部求出来，然后传播到第一层。

每轮训练，就是在网络上进行一次正向传播求得损失函数值，再进行一次误差梯度反向传播梯度下降更新参数。

具体做法不妨进行一些数学推导，我们直接考虑相邻两层的情况，形式化层间误差传播的过程。

假设神经网络的损失函数为 $E(\pmb x;\pmb \omega)$，为 $s$ 层网络，其中第 $t$ 层输出为

$$
{h}^{(t)}_j = f^{(t)}(\sum_{i = 1}^{m^{(t)}} \omega_{ji}^{(t)}h_{i}^{(t - 1)} + b_{j}^{(t)}), j = 1,2,\cdots,{m^{(t)}}
$$

而第 $t + 1$ 层的输出为

$$
{h}^{(t + 1)}_k = f^{(t + 1)}(\sum_{j = 1}^{m^{(t + 1)}} \omega_{kj}^{(t + 1)}h_{j}^{(t)} + b_{k}^{(t + 1)}), k = 1,2,\cdots,{m^{(t + 1)}}
$$

不妨设仿射函数

$$
z_j^{(t)} = \sum_{i = 1}^{m^{(t)}} \omega_{ji}^{(t)}h_{i}^{(t - 1)} + b_{j}^{(t)}
$$

$$
z_k^{(t + 1)} = \sum_{j = 1}^{m^{(t + 1)}} \omega_{kj}^{(t + 1)}h_{j}^{(t)} + b_{k}^{(t + 1)}
$$

要求 $\dfrac{\partial E}{\partial \omega_{ji}^{(t)}}, \dfrac{\partial E}{\partial b_{j}^{(t)}}$，即求

$$
\dfrac{\partial E}{\partial \omega_{ji}^{(t)}} = \dfrac{\partial E}{\partial z_j^{(t)}}\dfrac{\partial z_j^{(t)}}{\partial \omega_{ji}^{(t)}}
$$

$$
\dfrac{\partial E}{\partial b_{j}^{(t)}} = \dfrac{\partial E}{\partial z_j^{(t)}}\dfrac{\partial z_j^{(t)}}{\partial b_{j}^{(t)}}
$$

显然

$$
\dfrac{\partial z_j^{(t)}}{\partial \omega_{ji}^{(t)}} = h_{i}^{(t - 1)}
$$

$$
\dfrac{\partial z_j^{(t)}}{\partial b_{j}^{(t)}} = 1
$$

注意看，这个 $\dfrac{\partial E}{\partial z_j^{(t)}}$ 东西叫误差，就是可以复用的东西，我们不妨对它继续展开看看。

$$
\dfrac{\partial E}{\partial z_j^{(t)}} = \sum_{k = 1}^{m^{(t + 1)}}\dfrac{\partial E}{\partial z_k^{(t + 1)}}\dfrac{\partial z_k^{(t + 1)}}{\partial h_{j}^{(t)}}\dfrac{\partial h_j^{(t)}}{\partial z_j^{(t)}}
$$

> 注：可以统一把 $z$ 看做神经元输入，$h$ 看做输出，这样好理解一些。

出现了！是上层误差 $\dfrac{\partial E}{\partial z_k^{(t + 1)}}$！这就是我们想要的误差逆传播公式，它在每层都成立——先用损失函数对上层输入求导，上层输入再对该层输出求导，该层输出再对该层输入求导，因为对于该层任一神经元，上层每个输入都有来自该神经元的成分，所以有个求和（这是链式法则的基本法）。

把它整理一下，用

$$
\delta^{(t)}_j = \dfrac{\partial E}{\partial z_j^{(t)}}
$$

$$
\delta^{(t + 1)}_k = \dfrac{\partial E}{\partial z_k^{(t + 1)}}
$$

表示误差，得到

$$
\delta^{(t)}_j = \dfrac{\partial h_j^{(t)}}{\partial z_j^{(t)}} \sum_{k = 1}^{m^{(t + 1)}}\delta_k^{(t + 1)}\omega_{kj}^{(t + 1)}, j = 1,2,\cdots,m^{(t)}
$$

你高低可以把它写成一个矩阵乘法

$$
\pmb\delta^{(t)} = \dfrac{\partial h_j^{(t)}}{\partial z_j^{(t)}}
\left[
\begin{matrix}
    \omega_{11}^{(t + 1)} & \omega_{21}^{(t + 1)} &\cdots & \omega_{m^{(t + 1)}1}^{(t + 1)}\\
    \omega_{12}^{(t + 1)} & \omega_{22}^{(t + 1)} &\cdots & \omega_{m^{(t + 1)}2}^{(t + 1)}\\
    \vdots & \vdots &\ddots &\vdots\\
    \omega_{1m^{(t)}}^{(t + 1)} & \omega_{2m^{(t)}}^{(t + 1)} & \cdots & \omega_{m^{(t + 1)}m^{(t)}}^{(t + 1)}
\end{matrix}
\right]
\left[
\begin{matrix}
    \delta_1^{(t + 1)}\\
    \delta_2^{(t + 1)}\\
    \vdots\\
    \delta_{m^{(t + 1)}}^{(t + 1)}
\end{matrix}
\right]
$$

然后丢到 GPU 里算起来就很爽了。

> 注：
> （1）建议对照上面每个函数的表达式进行理解。
> （2）你可能要问偏置 $b$ 去哪了捏？注意偏置节点都是不与前层相连的，所以求导求导它这就没了捏。

对于每次前向传播完成的网络来说，$t$ 层神经元激活函数的导数 $\dfrac{\partial h_j^{(t)}}{\partial z_j^{(t)}}$ 和 $t + 1$ 层的权重 $\omega_{kj}^{(t + 1)}$ 都是已知的，所以这个东西实际上就是个递推公式。

得到误差，$t$ 层任意参数的梯度就好求了

$$
\dfrac{\partial E}{\partial \omega_{ji}^{(t)}} = \delta_{j}^{(t)}h_i^{(t - 1)}
$$

$$
\dfrac{\partial E}{\partial b_{j}^{(t)}} = \delta_{j}^{(t)}
$$

你大可以试着把这两个玩意的计算也合起来写成矩阵乘法，这样一来就跟正向传播的矩阵对上了，方便进行参数更新。

那么我们来总结一下求所有参数梯度的过程：首先必须进行一次正向传播（得到反向传播所需的必要的值）并计算损失，然后从输出层开始计算损失的误差 $\pmb\delta^{(s)}$，然后用上面这俩公式把第 $s - 1$ 层的所有参数梯度给求了，然后用 $\delta^{(s - 1)} \leftarrow \delta^{(s)}$ 的递推公式求第 $s - 1$ 层的误差，依次类推，直到把第一层的参数梯度都求完。这样一个过程，就是整个误差逆传播的过程，它可以完全用矩阵乘法的形式给出。

求完所有参数的梯度，直接梯度下降更新参数就得了，至此，一次训练也就结束了。

> 注：
> （1）这个地方还有一个关于输入的问题：可以输入一组，也可以输入多组数据，即随机梯度下降或批量梯度下降。这关系到参数更新的频率，如果每次训练只输入一组数据，那么在整个数据集做一轮训练就需要很多次参数更新，这未免有些浪费算力。所以一般我们采用 batch 方法，把数据集划分为多个 batches，每次计算一个 batch 贡献的平均梯度进行梯度下降。
> （2）上面的推导隐含了输入数据为一组的假设，实际上多组没什么区别，把列向量合并成矩阵，所有计算方法都是一样的。

## 待更

（1）实例和代码
（2）自动微分