# 感知机、神经元到全连接网络

## 写在前面

感知机是神经网络和支持向量机的基础，它是一个线性分类器，可以作为神经网络的一个最简单神经元（后续还有更复杂的神经元形式），也可以作为支持向量机的基础模型。

全连接网络是最简单的神经网络，是神经网络的基本范式，其它复杂的神经网络基本都是基于全连接网络发展而来的。

## 感知机

感知机是一个二分类线性分类模型，输入为样本的特征向量，输出为样本的类别，方法是在空间中找到一个将二类分离的超平面。为了寻求最优超平面的参数，仍然使用梯度下降法最小化关于误分类信息的损失函数。

### 感知机结构和定义

假设特征空间为 $\mathcal{X}\subset R^n$，输出空间 $\mathcal{Y} = \{1, -1\}$，样本特征向量 $\pmb{x}\in \mathcal X$，输出 $y\in \mathcal Y$ 表示样本类别。假设空间的一个函数

$$
f(x) = \text{sign}(\pmb\omega \cdot \pmb x + b)
$$

称为感知机。其中 $\pmb\omega（权值）, b$ （偏置）为模型参数，$\text{sign}$ 是符号函数。

![Screenshot-from-2022-11-11-04-37-32.png](http://image.tjzfile.xyz/images/2022/11/11/Screenshot-from-2022-11-11-04-37-32.png)

有了分类模型，接下来就要考虑如何进行模型学习了。首先必须考虑模型学习需要优化的损失函数，它必须易求且可微，然后就是考虑优化它的方法。

> 注：
> （1）线性可分：如果数据集中的正例和反例可以被一个超平面完全分离，那么这个数据集就是线性可分的。

### 感知机学习策略

由于我们必须找到一个关于参数 $\pmb\omega, b$ 可微的损失函数，所以将它们都用上是最好的方案，显然超平面在空间中的位置特性是与参数强相关的，故应首先考虑位置关系，那么距离是一个好的选择。

根据点到直线的距离公式，容易推得任意点到超平面的距离

$$
l(\pmb x_0) = \dfrac{|\pmb\omega\cdot\pmb x_0 + b|}{||\pmb\omega||}
$$

而且，对于误分类的数据 $(\pmb x_i,y_i)$，分类器所得类别与真实类别相反，即必然有

$$
-y_i(\pmb\omega\cdot\pmb x_i + b) > 0
$$

更进一步，由于任意 $|y_i| = 1$，所以误分类点到超平面的距离为

$$
-\dfrac{1}{||\pmb\omega||}y_i(\pmb\omega\cdot\pmb x_i + b)
$$

则所有误分类点到超平面的总距离

$$
-\dfrac{1}{||\pmb\omega||}\sum_{\pmb x_i\in M}y_i(\pmb\omega\cdot \pmb x_i + b)
$$

其中 $M$ 为所有误分类点的集合，它对于所有误分类点可导。

不考虑 $\dfrac{1}{||\pmb\omega||}$，取

$$
L(\pmb\omega;b) = -\sum_{\pmb x_i\in M}y_i(\pmb\omega\cdot \pmb x_i + b)
$$

作为感知机的损失函数。

### 感知机学习方法

由于批量梯度下降可能存在样本对损失的贡献相互抵消从而导致模型无法很好收敛的问题，感知机学习方法采用随机梯度下降，每轮只随机取一个样本进行梯度下降。

假设随机选取的误分类点为 $(x_i, y_i)$，直接求导可得

$$
\dfrac{\partial L}{\partial\pmb\omega} = -x_iy_i
$$

$$
\dfrac{\partial L}{\partial b} = -y_i
$$

那么每次参数更新为

$$
\pmb\omega \leftarrow \pmb\omega + \eta \pmb x_iy_i
$$

$$
b \leftarrow b + \eta y_i
$$

> 注：
>（1）可以期待在线性可分的数据集上，梯度下降一定会在有限步后收敛，得到一个分离超平面（Novikoff定理）。
>（2）感知机学习算法存在对偶形式，即将训练集各点误分类次数作为参数进行训练。

## 神经元模型

* 典型的神经元是MP神经元，它被用于全连接神经网络中，它通过一个仿射函数接收输入信号，经过一个激活函数变换后进行输出。

![R-C.jpg](http://image.tjzfile.xyz/images/2022/11/14/R-C.jpg)

神经元模型是感知机的推广，它可以被如下形式化描述。

假设特征空间为 $\mathcal{X}\subset R^n$，输出空间 $\mathcal{Y} = R^d$，样本特征向量 $\pmb{x}\in \mathcal X$，输出 $y\in \mathcal Y$ 表示样本类别。假设空间的一个函数

$$
f(x) = a(\pmb\omega \cdot \pmb x + b)
$$

称为感知机。其中 $\pmb\omega（权值）, b$ （偏置）为模型参数，$a$ 是激活函数。

其中

$$
z = \pmb\omega \cdot \pmb x + b = \sum_{i = 1}^n\omega_i x_i + b
$$

又称为仿射函数。注意到，一个神经元实际上就是**两个函数的复合的抽象模型**，这点很重要，是反向传播的数学基础。

> 注：
>（1）神经元不止MP神经元一种，它有相当多的种类，有各自不同的特殊功能和性质。
>（2）一个典型的激活函数是 Sigmoid 函数，它的表达式为 $f(z) = \dfrac{1}{1 + e^{-z}}$，将实数轴上的输入映射到 $(0, 1)$。

## 神经网络

* 神经网络就是用神经元组成的网络，超过两层的网络就被称为深度神经网络了。
* 神经网络在数学上看其实就是一个从特征空间映射到输出空间的非线性函数，这跟其他机器学习算法是一致的。
* 神经网络对给定输入给出一个输出，我们的目标是使得这个输出的概率分布与真实情况尽可能相近，因此为达成这个目标而进行的学习过程，实际上是一个最优化过程。

实际上神经网络有很多种，比如卷积神经网络和LSTM什么的，但是由于本质都是复合函数，所以在学习算法上是可以一致的。这里主要讲最简单的一种——全连接神经网络，或前馈神经网络，之所以说这种网络简单，是因为**它的结构相当规整，内部各子结构一致**。

> 注：有趣的是，前馈神经网络有一个万能逼近定理，这意味着它可以拟合逼近任意函数。要知道某种程度上一个人也可以看成函数，毕竟也可以简化成输入输出。

### 前馈神经网络

前馈神经网络是一种分层结构，每一层具有一些神经元，层内的神经元不互相连接，层与层的神经元才相互连接且全连接，但神经元的**响应在层间是单向传递的**，不会反过来激活，所以叫前馈神经网络。这跟人脑神经元连接结构就不太一样，在人脑中反馈激活/抑制的情况是普遍存在的。总之，其实不管哪种神经元，目前来看都只是对人脑神经元的拙劣模仿。

#### 二层前馈神经网络

* 先从最简单的神经网络说起，有助于理解神经网络的数学本质。

![webwxgetmsgimg.jpg](http://image.tjzfile.xyz/images/2022/11/14/webwxgetmsgimg.jpg)

可以看到，每个输出 $y_l$ 其实就是两层神经元的复合，它可以被严格地表示如下。

输入 $x_i, i\in[1, n]$，输出 $y_k, k\in[1, l]$，如图网络有两层（不包含输入层），第一层（隐藏层）有 $m$ 个神经元，其中第 $j$ 个的输出 $h_j^{(1)}$ 是

$$
h^{(1)}_j = f^{(1)}(\sum_{i = 1}^n\omega_{ji}x_i + b_j^{(1)}), j = 1, 2,\cdots, m
$$

这个神经元的输入就是 $\sum_{i = 1}^n\omega_{ji}x_i + b_j^{(1)}$，包括了输入层所有神经元的输出 $\{x_i\}_{i = 1}^n$ 和一个偏置 $b_j^{(1)}$，$f^{(1)}$ 是该层激活函数。
第二层（输出层）由 $l$ 个神经元组成，其中第 $k$ 个神经元的输出 $y_k$ 是

$$
y_k = f^{(2)}(\sum_{j = 1}^m\omega_{kj}h_j^{(1)} + b_k^{(2)}), k = 1,2,\cdots,l
$$

这个神经元的输入就是 $\sum_{j = 1}^n\omega_{kj}h_j^{(1)} + b_k^{(2)}$，包括了隐藏层所有神经元的输出 $\{h_j^{(1)}\}_{j = 1}^m$ 和一个偏置 $b_k^{(2)}$，$f^{(2)}$ 是该层激活函数。

注意到 $h_j^{(1)}$ 是第一层的输出，可见 $y_k$ 其实是四个函数的复合（两个仿射、两个激活函数）。

神经网络可以用矩阵表示，这使得它更易于计算，也更符合数学逻辑。

设特征向量 

$$
\hat{\pmb x} =
\left[ 
\begin{matrix}
    x_1\\
    x_2\\
    \vdots\\
    x_n\\
    1
\end{matrix}
\right]
$$

隐层输出

$$
\hat{\pmb h}^{(1)} =
\left[ 
\begin{matrix}
    h_1\\
    h_2\\
    \vdots\\
    h_m\\
    1
\end{matrix}
\right]
$$

输出层输出

$$
\pmb y =
\left[ 
\begin{matrix}
    y_1\\
    y_2\\
    \vdots\\
    y_l\\
\end{matrix}
\right]
$$

隐层权重

$$
\pmb W^{(1)} = 
\left[
    \begin{matrix}
        \omega_{11}^{(1)} &\omega_{12}^{(1)} & \cdots &\omega_{1m}^{(1)}\\
        \omega_{21}^{(1)} &\omega_{22}^{(1)} & \cdots &\omega_{2m}^{(1)} \\
        \vdots & \vdots &\ddots &\vdots \\
        \omega_{n1}^{(1)} &\omega_{n2}^{(1)} & \cdots &\omega_{nm}^{(1)} \\
        b_1^{(1)} & b_2^{(1)} & \cdots & b_m^{(1)}
    \end{matrix}
\right]
$$

输出层权重

$$
\pmb W^{(2)} = 
\left[
    \begin{matrix}
        \omega_{11}^{(2)} &\omega_{12}^{(2)} & \cdots &\omega_{1l}^{(2)} \\
        \omega_{21}^{(2)} &\omega_{22}^{(2)} & \cdots &\omega_{2l}^{(2)} \\
        \vdots & \vdots &\ddots &\vdots \\
        \omega_{m1}^{(2)} &\omega_{m2}^{(2)} & \cdots &\omega_{ml}^{(2)} \\
        b_1^{(1)} & b_2^{(1)} & \cdots & b_l^{(1)}
    \end{matrix}
\right]
$$

这样就有

$$
\hat{\pmb h} = f^{(1)}(\pmb W^{(1)^T}\hat{\pmb x})\\
\pmb y = f^{(2)}(\pmb W^{(2)^T} \hat{\pmb h}^{(1)})
$$

> 注：也可以将多个特征列向量组织成矩阵输入，相当于多个样本批量输入网络。

#### 多层前馈神经网络

多层前馈神经网络就是二层网络的简单堆叠，所以其实没什么好说的。

前馈神经网络一般形式如下。

网络输入 $x_i, i=1,2,\cdots,n$ 输出 $y_k, k = 1,2,\cdots,l$，设神经网络有有限的 $s$ 层（不计输入层），假设其中第 $t$ 层由 $m^{(t)}$ 个神经元组成，则第 $t$ 层的第 $j$ 个神经元的输出为

$$
\pmb h_j^{(t)} = f^{(t)}(\sum_{i = 1}^{m^{(t)}}\omega_{ji}h_{i}^{(t - 1)} + b_j^{(t)}), j = 1,2,\cdots,m
$$

前馈神经网络也可以写成矩阵的形式，这实际上也是实现神经网络的具体方法。

$$
\left\{
    \begin{aligned}
        &\pmb h^{(1)} = f^{(1)}(\pmb W^{(1)^T}\hat{\pmb x})\\
        &\pmb h^{(2)} = f^{(2)}(\pmb W^{(2)^T}\hat{\pmb h}^{(1)})\\
        &\vdots\\
        &\pmb h^{(s - 1)} = f^{(s - 1)}(\pmb W^{(s - 1)^T}\hat{\pmb h}^{(s - 2)})\\
        &\pmb y = \pmb h^{(s)} = f^{(s)}(\pmb W^{(s)^T}\hat{\pmb h}^{(s - 1)})\\
    \end{aligned}
\right.
$$

### 误差反向传播算法