# 模型评估与选择

## 基本概念

### 误差

* 有训练误差（经验误差）、测试误差（泛化误差）两种，顾名思义即可。

### 过拟合

选择或学习一个合适的模型，并且如果在假设空间中存在“真”模型，那么所选择的模型应该逼近真模型，但是一般情况下我们无法获取假设空间的全部数据用于拟合来逼近这个模型。对于假设空间中部分数据的拟合结果，如果它越接近这部分数据的分布，它越有可能具有更低的泛化能力。

* 训练误差小但测试误差大，就很有可能发生了过拟合。这是不那么令人满意的结果。
* 一般来说，过拟合的模型复杂度更高些（参数更多）。

下图中，绿线代表过拟合的模型，黄线代表欠拟合的模型，黑线是假想的真模型。

> 注：一般来说，过拟合的模型会在训练误差小到一定程度时，测试误差不断增大。

![Screenshot-from-2022-10-03-00-22-13.png](http://image.tjzfile.xyz/images/2022/10/03/Screenshot-from-2022-10-03-00-22-13.png)

* 通常通过正则化，或说是添加正则项来解决这个问题，正则化添加的惩罚与模型复杂度成正比，使得模型选择使经验风险和结构风险同时最小的模型。


### 精度

通常通过如下式子计算

$$
acc = \dfrac{1}{m}\sum_{i = 1}^m I(y_i = f(\pmb x_i))
$$

当 $y_i = f(x_i)$ 时，$I$ 为 $1$，否则为 $0$。

## 正则化

正则化是结构风险最小策略的实现，在正常的损失函数上加一个惩罚项就是。

$$
E(Y, f(X)) = \sum_{i = 1}^m L(Y, f(X)) + \lambda g(f)
$$

$\lambda||f||_l$ 就是惩罚项，或称为正则项（其中 $\lambda$ 为惩罚系数，是常数）。一般来说，正则项 $g(f)$ 是一个关于模型 $f$ 的复杂度的单调递增函数。

前面提到，模型的复杂度的一个显性表征即是模型参数的个数，因此不妨设 $g(f) = ||\pmb {\hat\omega}||_l$，即参数向量的 $l$ 范数。

向量的各阶范数分别有以下特征：

* $0$ 范数：向量中非 $0$ 元素的个数。
* $1$ 范数：向量中各个元素的绝对值之和。
* $2$ 范数：向量中各个元素的平方和。

> 注：选取 $0, 1$ 范数使得 $\pmb{\hat\omega}$ 的分量中 $0$ 分量尽可能多，$2$ 范数使得各分量更均匀。

添加二范数正则项，将使多元线性回归问题变得一定可解。
## 验证方法

* 为了避免过拟合，也可以在训练集上做一些工作，即在训练过程中引入验证集用以优化参数。
* 在学习到的不同模型中，选择对验证集有最小预测误差的模型，这也意味着这些验证方法往往要求训练多次。

> 注：由于实际上并没有在验证集上进行训练，验证集的评估结果是作为测试误差来看待的。

### 留出法

* 直接将训练集拆成两个互斥集部分。这种方法往往是最常用的。
* 为了更好的训练效果，往往还需要满足数据乱序、样本按权平均分配的条件，这保证了数据分布的一致性。

一般直接通过乱序分层采样解决这一问题，简单来说就是调个api打乱一下数据然后按样本各类别比例均匀分配数据至训练集和测试集。比如，若正例和反例各占 $50%$，那么对于 $1000$ 个样本的 $80\%:20\%$ 划分，训练集应分到 $400$ 个正例，$400$ 个反例。

贴个代码，数据集应该是[这个](https://www.kaggle.com/datasets/brendan45774/test-file)，但是经过了预处理。

```python
import pandas as pd
import numpy as np

train_ti_buf = pd.read_csv('train_titanic.csv')
test_ti_buf = pd.read_csv('test_titanic.csv')

train_ti = np.array(train_ti_buf[['Age', 'Fare', 'Sex', 'Pclass', 'Survived']])
# split the data into two class
train_ti_0 = train_ti[np.where(train_ti[:, -1] == 0)]
train_ti_1 = train_ti[np.where(train_ti[:, -1] == 1)]

# shuffle the datasets
np.random.shuffle(train_ti_0)
np.random.shuffle(train_ti_1)

# rebuild the datasets
data_cardinaty = np.array([len(train_ti_0), len(train_ti_1)])
tune = data_cardinaty[:] // 5
train_ti = np.vstack((train_ti_0[tune[0]:], train_ti_1[tune[1]:]))
validation_ti = np.vstack((train_ti_0[:tune[0]], train_ti_1[:tune[1]]))

# you can just shuffle again
np.random.shuffle(train_ti)
np.random.shuffle(validation_ti)
```

### 交叉验证法

* 将数据集分层采样划分为 $k$ 个大小相似的互斥子集，每次用 $k - 1$ 个子集作为训练集，剩下的作为验证集。
* 在 $k$ 种不同的训练集-验证集选择下训练 $k$ 次，取各次验证集评估结果的平均值作为最终结果返回。这称为 $k$ 折交叉验证，一般 $k = 10$ 如下图。

![Screenshot-from-2022-10-03-10-45-52.png](http://image.tjzfile.xyz/images/2022/10/03/Screenshot-from-2022-10-03-10-45-52.png)

* 若对以上步骤反复执行 $p$ 次，得到 $p$ 种数据集的划分，则每一种划分方法对应一次交叉验证，最终在 $p$ 次交叉验证返回的结果中取最优的那次对应的模型作为最终模型。这称为 $p$ 次 $k$ 折交叉验证。

> 注：$p$ 次 $k$ 折交叉验证实际上对数据集进行了 $p \times k$ 次训练。

贴一个乱写的交叉验证，有一点长。

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

train_ti_buf = pd.read_csv('train_titanic.csv')
test_ti_buf = pd.read_csv('test_titanic.csv')

train_ti = np.array(train_ti_buf[['Age', 'Fare', 'Sex', 'Pclass', 'Survived']])
train_titan_ = train_ti
np.random.shuffle(train_ti)
data_cardinaty = np.shape(train_ti)[0]
batch_size = data_cardinaty // 10
batches = data_cardinaty // batch_size
class_num = np.shape(train_ti)[1]

eta = 1e-2
epochs = 10

train_ti = np.c_[np.ones((np.shape(train_ti)[0], 1)), train_ti]
train_ti = np.array_split(train_ti, batches)

times = 10
history = []
omega_ti = np.zeros(class_num, dtype = float)

for epoch in range(epochs):
    history_ = []
    train_titan = train_titan_
    np.random.shuffle(train_titan)
    train_titan = np.c_[np.ones((np.shape(train_titan)[0], 1)), train_titan]

    train_titan_0 = train_titan[np.where(train_titan[:, -1] == 0)]
    train_titan_1 = train_titan[np.where(train_titan[:, -1] == 1)]

    data_cardinaty = np.array([len(train_titan_0), len(train_titan_1)])
    tune = data_cardinaty[:] // batches
    train_titan = np.array(train_titan[-1])
    for i in range(10):
        train_titan = np.vstack((train_titan, train_titan_0[tune[0] * i: tune[0] * (i + 1)]))
        train_titan = np.vstack((train_titan, train_titan_1[tune[1] * i: tune[1] * (i + 1)]))
    

    train_titan = np.array_split(train_titan, batches)
    
    for time in range(times):
        train_dataset = np.delete(train_titan, [time,])
        validation_dataset = train_titan[time]

        max_steps = 3
        mini_batch = train_dataset[np.random.randint(batches - 1, size = (5))]
    
        for batch in mini_batch:
            for step in range(max_steps):
                omega_ex = [np.sum([point[j] * (np.exp(np.sum(omega_ti * point[:class_num])) / \
                           (1 + np.exp(np.sum(omega_ti * point[:class_num]))) - point[-1]) for point in batch]) / np.shape(batch)[0] for j in range(class_num)]
    
                omega_ti -= np.array(eta) * omega_ex
        
        size_val = np.shape(validation_dataset)[0]
        prediction_val = np.array([(1 / (1 + np.exp(-np.sum(omega_ti * x[:class_num])))) > 0.5 for x in validation_dataset])
        acc_val = 1 / size_val * np.sum([1 if prediction_val[i] == validation_dataset[i][-1] else 0 for i in range(size_val)])
        history_.append(acc_val)
 
    history.append((omega_ti, np.mean(history_)))

    train_dataset = train_dataset[np.random.randint(0, batches - 1)]
    size_t = np.shape(train_dataset)[0]
    size_val = np.shape(validation_dataset)[0]

    prediction = np.array([(1 / (1 + np.exp(-np.sum(omega_ti * x[:class_num])))) > 0.5 for x in train_dataset])
    acc = 1 / size_t * np.sum([1 if prediction[i] == train_dataset[i][-1] else 0 for i in range(size_t)])
    loss = -1 / size_t * np.sum([np.log(point[-1] / (1 + np.exp(-np.sum(omega_ti * point[:class_num]))) + \
    (1 - point[-1]) * np.exp(-np.sum(omega_ti * point[:class_num])) / (1 + np.exp(-np.sum(omega_ti * point[:class_num])))) for point in train_dataset])
    
    print(f'epoch: {epoch}\ttrain acc: {acc:.4}, train loss: {loss:.4}, validation acc: {history[epoch][1]:.4}')
print(omega_ti)
print(f'acc: {np.mean(history, axis = (0))[1]}')
```

### 自助法

* 自助法就是把从训练集中**放回抽样**任意的 $m$ 次后得到的集合作为验证集。

## 性能度量和分析

### 查准率、查全率和ROC曲线

* 查准率 $P$：被模型判断为正例的数据条目中，真实类别也是正例的条目所占的比率。
* 查全率 $R$：真实类别为正例的数据条目中，被模型判断为正例的条目所占的比率。

> 注意：对于不同的模型分类阈值（一般是 $0.5$ ），查全率和查准率不同。

先把所有经过训练的数据条目按如下方式划分为互斥的四个集合，方便讨论。

![Screenshot-from-2022-10-03-13-34-25.png](http://image.tjzfile.xyz/images/2022/10/03/Screenshot-from-2022-10-03-13-34-25.png)

那么得到计算公式

$$
P = \dfrac{TP}{TP + FP}
$$

$$
R = \dfrac{TP}{TP + FN}
$$

![Screenshot-from-2022-10-03-13-37-41.png](http://image.tjzfile.xyz/images/2022/10/03/Screenshot-from-2022-10-03-13-37-41.png)

计算 $P,R$ 的算法也非常简单，复杂度 $O(n)$，其中 $n$ 为数据条数。

* 对训练数据根据正例置信度从大到小排序，从前往后遍历。
* 将每个数据条目的置信度尝试作为阈值，计算这种情况的查全率和查准率。
* 绘制 $PR$ 曲线，一般长下面这样。

![Screenshot-from-2022-10-03-13-49-21.png](http://image.tjzfile.xyz/images/2022/10/03/Screenshot-from-2022-10-03-13-49-21.png)

> 注：
> （1）分类器设定较高阈值时，查准率高，查全率低；设定较低阈值时，查准率低，查全率高。
> （2）查全率为横坐标，查准率为纵坐标。

贴个代码。

```python
test_ti = np.array(test_ti_buf[['Age', 'Fare', 'Sex', 'Pclass', 'Survived']])
size_test = np.shape(test_ti)[0]

test_ti = np.hstack((np.ones((size_test, 1)), test_ti))

test_PR = []
prediction = np.array([((1 / (1 + np.exp(-np.sum(omega_ti * x[:class_num])))), x[-1]) for x in test_ti])
prediction = sorted(prediction, key = lambda x : x[0], reverse = True)

for i in range(1, size_test):
    test_tp = np.sum([prediction[x][1] == True for x in range(i)])
    test_fp = np.sum([prediction[x][1] == False for x in range(i)])
    test_fn = np.sum([prediction[x][1] == True for x in range(i + 1, size_test)])
    test_PR.append((test_tp / (test_fp + test_tp), test_tp / (test_tp + test_fn)))

test_PR = np.reshape(test_PR, (-1, 2))
plt.scatter(test_PR[..., 1], test_PR[..., 0], s = 1)
plt.plot(test_PR[..., 1], test_PR[..., 0])
x = np.linspace(0, 1)
y = x
plt.plot(x, y)
plt.grid()
```

#### 拓展

比 $PR$ 曲线更常用的是 $F1$ 度量。

$$
F1 = \dfrac{2\times P\times R}{P + R} = \dfrac{2\times TP}{2\times TP + FN + FP}
$$

比 $F1$ 更一般的是 $F_\beta$：

$$
F_\beta = \dfrac{(1 + \beta^2)\times P\times R}{(\beta^2 \times P) + R}
$$

### ROC曲线

* 真正例率：在真实模型中是正例的数据条目，模型预测也是正例的条目所占的比率。
* 假正例率：在真实模型中是反例的数据条目，模型预测却是正例的条目所占的比率。

![Screenshot-from-2022-10-03-18-26-52.png](http://image.tjzfile.xyz/images/2022/10/03/Screenshot-from-2022-10-03-18-26-52.png)

用与 $P,R$ 同样的方法计算它们，也可以绘制一张图，即 $ROC$ 曲线。

![Screenshot-from-2022-10-03-18-30-43.png](http://image.tjzfile.xyz/images/2022/10/03/Screenshot-from-2022-10-03-18-30-43.png)

曲线下方的面积称为 $AUC$ 值。

* $ROC$ 曲线越凸，模型性能越好。因为模型预测得到的结果在正确类别上的平均置信度相对较高。
* $AUC$ 值越大，模型性能越好。原理与 $ROC$ 曲线凸性基本相同。

## 附录

* 真正例率和假正例率的计算方法

![Screenshot-from-2022-10-03-18-38-23.png](http://image.tjzfile.xyz/images/2022/10/03/Screenshot-from-2022-10-03-18-38-23.png)