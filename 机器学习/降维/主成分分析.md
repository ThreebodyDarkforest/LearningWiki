# 主成分分析

## 维数灾难

从一般常理出发来思考，刻画某个概念的信息越多，这个概念就越清晰。但在机器学习中，这一观点不总是对的，或者说我们应该以更严格的角度去思考。我们需要考虑两个问题：

- 信息越多，一定越有用吗？
- 一个概念需要多少信息来描述？

在机器学习中，信息太多常常意味着特征空间的维度较高，此时的算力开销就会相当大。特征维数越多，就意味着需要学习的模型越复杂，拟合越困难，那么训练模型需要的数据量也会越多。此外，许多机器学习方法都涉及距离计算，当特征维度太多时，距离计算将变得相当复杂。

再来看第二个问题，我们从现实数据中收集到的原始数据常常含大量冗余，但其实含有许多影响不大的特征维度。那么我们是否可以用类似信号处理中低通滤波的方式，将这些数据中的主要部分提取出来，达到**降维**的目的而又保持原始数据的本质结构呢？答案是肯定的，而且这就是主成分分析的主要思路。

> 注：数据样本虽然是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维嵌入（embedding）。embedding 是机器学习领域的一个重要方法。

## 样本主成分分析

样本主成分分析顾名思义，就是对数据样本中的成分（维度）进行分析，并提取出其中的主要成分（维度）。这一方法通过找出特征空间中最能表征数据分布的几个线性无关的新单位向量代替原有维度的基向量，或者说他把由线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据。具体来说，它使用正交变换和凸优化达到这一点，其中线性无关的变量称为主成分。

> 注：对于给定数据集，特征空间不同的维度就对应数据集不同的特征，它们不一定都是正交的。