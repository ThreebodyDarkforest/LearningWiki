# 主成分分析

## 维数灾难

从一般常理出发来思考，刻画某个概念的信息越多，这个概念就越清晰。但在机器学习中，这一观点不总是对的，或者说我们应该以更严格的角度去思考。我们需要考虑两个问题：

- 信息越多，一定越有用吗？
- 一个概念需要多少信息来描述？

在机器学习中，信息太多常常意味着特征空间的维度较高，此时的算力开销就会相当大。特征维数越多，就意味着需要学习的模型越复杂，拟合越困难，那么训练模型需要的数据量也会越多。此外，许多机器学习方法都涉及距离计算，当特征维度太多时，距离计算将变得相当复杂。

再来看第二个问题，我们从现实数据中收集到的原始数据常常含大量冗余，但其实含有许多影响不大的特征维度。那么我们是否可以用类似信号处理中低通滤波的方式，将这些数据中的主要部分提取出来，达到**降维**的目的而又保持原始数据的本质结构呢？答案是肯定的，而且这就是主成分分析的主要思路。

> 注：数据样本虽然是高维的，但与学习任务密切相关的也许仅是某个低维分布，即高维空间中的一个低维嵌入（embedding）。embedding 是机器学习领域的一个重要方法。

## 样本主成分分析

样本主成分分析顾名思义，就是对数据样本中的成分（维度）进行分析，并提取出其中的主要成分（维度）。这一方法通过找出特征空间中最能表征数据分布的几个线性无关的新单位向量代替原有维度的基向量，或者说他把由线性相关变量表示的观测数据转换为由少数几个线性无关变量表示的数据。具体来说，它使用正交变换和凸优化达到这一点，其中**线性无关的变量称为主成分**。

> 注：对于给定数据集，特征空间不同的维度就对应数据集不同的特征，它们不一定都是线性无关的。

简单地说，我们需要找出一个与原坐标系维数相同的新的正交坐标系，在该坐标系中只需要少数几个主要维度即可很好地表达样本信息。

我们的要求很简单：

- 新的坐标系只需要少数几个维度就能够很好地表达原坐标系的信息。
- 新的坐标系的基向量必须两两正交（线性无关）。

对于第一个要求，我们通过最大化**规范化的**数据在所有新坐标系维度上的取值的方差来实现（在规范化的数据中，等价于最小化数据到新坐标轴的距离的平方和）。对于第二个要求，假设新坐标系的所有基向量为 $A = (\pmb\alpha_1, \pmb\alpha_2, \cdots,\pmb\alpha_m)^T$，我们只需添加一个约束 $A  A^T = I$ 即可。

> 注：
> （1）规范化的样本即在所有维度上都有均值为 $0$，方差为 $1$ 的样本，这是为了避免不同维度的尺度·量纲不同对主成分分析的结果造成影响。
> （2）所有基向量两两正交，那么这组基向量是标准正交基。
> （3）完全可以将一个主成分视为降维后的新坐标系的其中一个维度，所谓第 $k$ 主成分其实就是数据降维后的第 $k$ 维的取值。

接下来我们来看看第一个要求中的方差怎么表示。

设 $X\in R^m$ 为输入数据，共 $n$ 个样本，其中 $x_{ij}$ 表示第 $j$ 个样本的第 $i$ 个维度的取值

$$
X = [x_{ij}]_{m\times n} = \left[
\begin{matrix}
    x_{11}& x_{12} &\cdots &x_{1n}\\
    x_{21}& x_{22} &\cdots &x_{2n}\\
    \vdots &\vdots &\ddots &\vdots\\
    x_{m1}& x_{m2} &\cdots &x_{mn}
\end{matrix}
\right]
$$

设第 $i$ 维的样本均值

$$
\bar x_{i} = \dfrac{1}{n - 1}\sum_{j = 1}^m x_{ij}
$$

那么协方差矩阵

$$
\Sigma = [Cov(x_k, x_l)]_{m\times m}
$$

其中

$$
Cov(x_k, x_l) = \dfrac{1}{n - 1}\sum_{j = 1}^n(x_{kj} - \bar x_k)(x_{lj} - \bar x_l)
$$

表示样本第 $k$ 维与第 $l$ 维的协方差。

首先我们需要得知某个数据样本 $\pmb x_j$ 在新坐标系下的坐标表示 $\pmb y_j = (y_{1j},y_{2j},\cdots,y_{mj})^T$，显然，这不过是一个正交变换

$$
\pmb y_j =  A \pmb x_j = (\pmb\alpha_1^T\pmb x_j, \pmb\alpha_2^T\pmb x_j,\cdots,\pmb \alpha_m^T\pmb x_j)^T
$$

那么第 $i$ 维的的样本均值

$$
\bar y_{i} = \dfrac{1}{n - 1}\sum_{j = 1}^m y_{ij} = \pmb\alpha_i^T\bar x_i
$$

于是第 $i$ 维的方差为

$$
s_{ii} = \dfrac{1}{n - 1}\sum_{j = 1}^n(y_{ij} - \bar y_i)^2 \\= \pmb\alpha_i^T[\dfrac{1}{n - 1}\sum_{j = 1}^m(x_{ij} - \bar x_i)^2]\pmb \alpha_i = \pmb\alpha_i^T\Sigma\pmb \alpha_i
$$

所以协方差

$$
Cov( y_k,  y_l) = \pmb\alpha_k^T\Sigma\pmb \alpha_l
$$

接下来我们只需要在约束条件下最大化方差即可，于是问题可以被定义为

$$
\max \text{tr}( A^T \Sigma A)\\
st.~  A A^T = I
$$

注意到当样本被规范化后，有

$$
\Sigma = \dfrac{1}{n - 1}X X^T
$$

那么问题变为

$$
\max \text{tr}( A^T XX^T A)\\
st.~  A A^T = I
$$

通过应用拉格朗日乘子法，我们就可以求得问题的解。

> 注：此处最大化的其实是数据在新坐标系所有维度之间的协方差（可以拆开这个硕大的连乘矩阵看一下），但是由于约束条件要求所有维度的基向量必须相互正交，那么 $Cov(y_k, y_l) = 0, k\neq l$，所以该问题实际上是在最大化所有维度上的方差。

---

上结论。

先对样本进行规范化，同样记规范化后的输入数据矩阵为 $X$

$$
x_{ij} = \dfrac{x_{ij} - \bar x_i}{\sqrt {s_{ii}}}
$$

其中 $s_{ii}$ 是所有样本在第 $i$ 维的方差，$\bar x_i$ 是第 $i$ 维的样本均值。

设 $\pmb\Sigma$ 是 $X$ 的协方差矩阵，$\pmb\Sigma$ 的特征值是 $\lambda_1\ge\lambda_2\ge\cdots\ge\lambda_m$，对应的特征向量是 $\pmb\alpha_1,\pmb\alpha_2,\cdots,\pmb\alpha_m$，则样本 $\pmb x_j$ 的第 $k$ 主成分是

$$
y_{kj} = \pmb\alpha^T_k \pmb x_j\\
k = 1, 2, \cdots,m, ~ j = 1,2,\cdots, n
$$

第 $k$ 主成分的方差是

$$
var(\pmb y_{k}) = \pmb\alpha_k^T\pmb \Sigma \pmb\alpha_k = \lambda_k
$$

> 注：
> （1）特征向量（系数向量）$\alpha_i$ 是单位向量。
> （2）不同主成分（维度）之间互不相关，即 $Cov(y_k, y_l) = 0, k\neq l$。