# 决策树

## 写在前面

决策树这东西，应该是最接近人类的线性逻辑推断方式的算法之一，或者说深度优先搜索本来就是一种惯用的思维模式，所以这算法其实是相当符合直觉的。

我们不妨来仔细研究一下人在对具有某些属性的未知事物作出经验判断的过程。首先可以肯定的是，判断的基本依据是我们曾经获得的知识，然后根据我们对未知物的各属性的了解程度为序，先从我们最了解的属性入手，抽丝剥茧缩小问题范围，最终确定一个最有可能的答案。比如我们遇到一个人，他每天都打游戏不认真听课，还经常错过各种重要活动，那么我们会认为这个人成绩不好的可能性是很高的，哪怕他可能成绩很好。

实际上决策树很大程度上就是在模拟这一过程，「判断的基本依据」在决策树中就是树的具体点边结构（抽象知识），其中「未知物的各属性名」即结点值，「未知物的某属性的取值」是边权，而「对未知物各属性了解程度」就是信息熵的表征，沿着这颗树从根走到叶子的一条路径就是关于某未知物的一个论断。

## 决策树的组成

### 前置概念

拿一张图来说省事些也好理解些。

![Screenshot-from-2022-11-06-17-11-29.png](http://image.tjzfile.xyz/images/2022/11/06/Screenshot-from-2022-11-06-17-11-29.png)

* 属性 $a$：第一行的除最后一列的内容。
* 属性取值 $a_v$：一种属性的可能取值集合是该属性所在列的不重复内容集合（除第一行），每个属性 $a$ 有一组可能的取值 $\{a_v\}_{v = 1}^{V}$。
* 类别：最后一列的内容。
* 类别集合 $\mathcal{Y}$：数据集中所有不重复取值的类别构成的集合。
* 问题：在一组数据集 $D$（即上图中的某些行的集合）中，找到某种规律判定每个数据样本的类别。
* 问题范围：问题对应数据集的大小 $|D|$。

### 决策树构造

* 决策树可能是多叉树（ID3/C4.5）也可能是二叉树（CART），但是都基于树这种基本数据结构，具有结点和边。
* 此外，这棵树是多模态的，它的叶子和非叶子所带信息种类是不同的。
  * 每个非叶子节点对应于某个属性，对于该属性的每一种可能取值引出一条边生成子树，这个子树代表着在该取值条件限定下的被缩小的问题范围。
  * 那么，根节点代表着总问题。
  * 每个叶子节点都是不可缩小的问题范围，在这个范围内决策树的知识无法再将其继续缩小（有三种情况），因此在此处决策树必须给出关于该问题的一个论断，该论断确定这叶子对应的类别。
* 这棵树的某条边代表这条边对应父节点属性的一个取值。

![Screenshot-from-2022-11-06-16-02-44.png](http://image.tjzfile.xyz/images/2022/11/06/Screenshot-from-2022-11-06-16-02-44.png)

> 注：
> （1）每一条从根节点到叶子的路径表示一个论断，即依次满足路径上属性取值的未知物为叶子类别。
> （2）决策树既可以做分类又可以做回归，因为如果把决策树形式化为图形，它可以表示一条折线。
> （3）决策树的训练集跟其他机器学习算法没什么不同，也可以看做特征向量。

这是决策树的一个相对具体的例子，应该与上文的叶子和非叶子节点、边进行对应比较理解。

![Screenshot-from-2022-11-06-16-10-07.png](http://image.tjzfile.xyz/images/2022/11/06/Screenshot-from-2022-11-06-16-10-07.png)

## 分析问题

* 在了解了决策树的原理和构造后，我们便要着手考虑如何生成（学习）一颗决策树。显然我们希望生成的决策树尽可能最优，即它应该与训练数据矛盾较小同时泛化能力较强。
* 当然，我们的起始问题是根节点，已知的信息是训练集。

不妨先分析一下需要解决的问题：

（1）如何选择一个非叶子节点的属性才是合理的？
（2）何时判定一个节点为叶子节点？
（3）如何根据（1）（2）建立树形结构？

为了解决问题（1），我们引入了信息熵和基尼系数作为判断依据。对于问题（2），我们只需要解决三种情况。

（3）问题是简单的，这就是数据结构课上的 DFS 建树，只不过条件更复杂。这条件简单地说，就是在 DFS 生成每个节点时都考虑（1）（2）。

### 选择非叶子节点属性

#### 信息增益和信息增益率

* 选择非叶子节点属性的依据基于这样的信息论理论：信息熵越大，信息量越大，未知物的不确定性越大。如果信息熵减小，那么未知物的不确定性减小。

首先明确我们选择一个非叶子节点属性的目标，就是要让子问题的**不确定性减少**地越多越好。具体的说就是要使得根据该属性取值划分的子问题范围的数据的类别尽可能一致，即**信息熵减小**。

能够让子问题的不确定性减少得最多的属性，一定是当前最有力的判据。举个例子。

>还是用上文的例子，我们选取两个属性：打游戏的频率和刷牙的频率。当我们得知一个人天天打游戏时，我们几乎可以断定他成绩不好；但如果我们只知道他每天不刷牙，那么认为他成绩不好是不如前者恰当的。如果你知道一些人的这两种属性而需要判断他们的成绩，那么前者一定是重要一些的。

在这个例子中，打游戏频率这个属性让难以分辨的人群得到了一个比较清晰的分类（打游戏的更多在成绩好类中，不打的更多在成绩好中），这就是不确定性的减少。

所以，我们只需要度量**用不同属性划分训练集得到的子问题信息熵总和并与划分前的信息熵作差**，即可得到不确定性减少的数学表达，这就是信息增益。而信息增益最大的属性，就是使得不确定性减少最多的属性。

---

解释地差不多了，那么就着手将其形式化为数学表达。注意其实信息熵的原本定义跟决策树的是有一点点不一样的。

* 信息量

一个随机事件的信息量，与它发生概率的大小有关，发生概率越大该事件确定性越大，信息量越小。在决策树中，我们把数据集中不同类别出现的频率作为概率的度量。

假设 $D$ 中有 $n$ 个不同类别的子集 $\{C_k\}_{k = 1}^n$，那么

$$
p_k = \dfrac{|C_k|}{|D|}
$$

$$
I_k = -\log_2 p_k
$$

* 信息熵

信息熵是样本信息量的期望，是整个样本不确定性程度的度量，信息熵越大，不确定性越大。如果数据集中各个类别出现的频率都差不多，那么这个数据集的不确定程度就很大。

$$
Ent(D) = -\sum_{k = 1}^n p_k\log_2 p_k
$$

![Screenshot-from-2022-11-06-19-14-15.png](http://image.tjzfile.xyz/images/2022/11/06/Screenshot-from-2022-11-06-19-14-15.png)

> 注：
> （1）$Ent(D)$ 越小，样本纯度越高。
> （2）若 $p_k = 0$，则 $p_k\log_2 p_k = 0$。
> （3）信息量具有可加性。

* 信息增益

属性 $a$ 有 $V$ 个可能的取值，即 $\{a_v\}_{v = 1}^V$，记在数据集 $D$ 上属性 $a$ 取值 $a_v$ 的子集为 $D_v$，则信息增益

$$
Gain(D, a) = \underset{划分前的信息熵}{\underline{Ent(D)}} - \underset{划分后的信息熵}{\underline{\sum_{v = 1}^{V}\dfrac{|D_v|}{|D|}Ent(D_v)}}
$$

我们在所有备选属性中取信息增益最大的作为当前节点的取值即可，ID3算法使用了信息增益。

> 注：
> （1）信息熵和信息增益计算的全过程实际上并不关心属性具体是什么，只关心训练集上属性不同取值的**类别比例**。
> （2）信息增益对取值数目多的属性存在偏好，因为这属性对问题刻画更详细，信息熵更有可能较小。

* 信息增益率

信息增益率希望消除信息增益对取值数目多的属性的偏好，于是它添加了一个关于属性 $a$ 可取值的惩罚项 $IV(a)$，其实就是属性集合的信息熵（上面那个是类别集合的信息熵），它越大，类别集合对减熵的贡献越小。

$$
IV(a) = -\sum_{v = 1}^V\dfrac{|D_v|}{|D|}\log_2\dfrac{|D_v|}{|D|}
$$

$$
Gain\_ratio(D, a) = \dfrac{Gain(D, a)}{IV(a)}
$$

和增益一样，我们需要在所有备选属性中选择增益率最大的。C4.5算法使用了信息增益率。

> 注：信息增益和增益率都是非负的，参考[这里](https://stackoverflow.com/questions/3289589/can-the-value-of-information-gain-be-negative#:~:text=Sure%20it%20can.%20Information%20gain%20is%20just%20the,in%20either%20direction--it%20can%20be%20positive%20or%20negative.)。

#### 基尼指数

* 基尼值

基尼值是度量样本纯度最常用的一种指标，下式中 $\mathcal{Y}$ 表示数据集中所有类别的集合，$p_k$ 就是取到第 $k$ 类的概率。

$$
Gini(D) = \sum_{k = 1}^{\mathcal{ |Y| }}\sum_{k'\neq k}p_kp_{k'} =  1 - \sum_{k = 1}^{|\mathcal{Y}|} p_k^2
$$

基尼值定义为在数据集中任取两个样本类别不一致的概率，基尼值越小，样本纯度越高。

* 基尼指数

基尼系数是以某个属性 $a$ 的不同取值 $a_v$ 划分数据集得到的各子集 $D_v$ 的基尼值的加权和，体现了划分属性后的平均样本纯度。

$$
Gini\_index(D, a) = \sum_{v = 1}^V \dfrac{|D_v|}{|D|}Gini(D_v)
$$

因为基尼指数越小样本纯度越高，所以我们需要在所有备选属性中选择使得划分后基尼指数最小的属性作为当前节点值。

### 判定叶子节点

什么时候应该停止继续引出分支节点呢？不难想到两种情况：要么就是现在属性没法对未知物进行进一步判断了，要么就是类别没法对未知物进行进一步判断了。毕竟我们总共就只用了这两个东西来进行判断。

进一步考虑，对于属性来说，又有三种情况：

（1）如果在当前节点已经用尽了可以用于判断类别的属性，那么显然无法继续了。
（2）如果在当前节点所有样本在节点的属性上都取同样的值，那么就没法进行属性划分了，也就没法引出分支了，也无法继续。
（3）如果在当前节点的子问题（即满足根节点到它的路径上的所有属性取值的数据子集）在节点属性上都不取某个属性值，那这个属性值就没法继续判断了。

> 注：关于（3），有必要指出它其实不太符合直觉，或者说不符合子问题的定义，但对于模型的泛化能力是很重要的。
> 按理来说由于这个子问题已经不包含某种可能的属性取值 $a_{v*}$ ，那么应该不考虑它才对，即直接不引出 $a_{v*}$ 分支结点。但是这样一来这个模型就只会在训练集上严格成立。考虑一个满足根到该节点路径的新数据，它在这节点取值 $a_{v*}$，那么模型就无法判断它的类别了。

对于类别来说，就只有一种情况：当前子问题的所有样本类别都相同，那我们就可以直接作一个判定。

## 算法步骤

结合问题分析中三个问题的解决方法，我们就可以写出算法步骤了。首先可以肯定的是基本算法当然是 DFS，从根节点开始，在每个节点上都考虑（1）（2）问题，然后用节点的不同属性取值生成新的节点。

![v2-ae7c8fc259ecae2399eda9a7fa85b2ab_r.jpg](http://image.tjzfile.xyz/images/2022/11/08/v2-ae7c8fc259ecae2399eda9a7fa85b2ab_r.jpg)

ID3、C4.5、CART的不同点就只有第（10）步选择最优划分属性（节点取值）的过程。

## 参考代码

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

def entropy(label):
    '''
    给定一个标记数组，计算该数组信息熵。

    @param: 
      label: 标记数组
    @return: 
      ent: 信息熵
    '''
    _label = np.array(label).flatten()
    _counter = Counter(_label)
    _sum = len(_label)
    ent = np.sum([-cnt / _sum * np.log2(cnt / _sum) for cnt in _counter.values()])
    return ent

def split(feature, label, dimension):
    '''
    根据选定属性维度 dimension 的不同属性取值对当前数据集进行子集划分。

    @param:
      feature: 数据集所有特征列
      label: 数据集标记列
      dimension: 属性维度
    @return:
      _res_1: 划分后的特征列子集集合
      _res_2: 划分后的标记列子集集合
    '''
    _container = np.array(sorted(np.c_[feature, label], key = lambda x : x[dimension]))    
    _counter = np.cumsum(list(Counter(np.array(_container[:, dimension])).values()))
    _res_1, _res_2 = np.split(_container[:, :-1], _counter[: -1]), np.split(_container[:, -1], _counter[:-1])
    _res_1, _res_2 = [x.tolist() for x in _res_1], [np.array([[v] for v in x]).tolist() for x in _res_2]
    return _res_1, _res_2

def Gain(feature, label, dimension):
    '''
    计算信息增益。
    '''
    res = entropy(label)
    ret = split(feature, label, dimension)
    for _labels in ret[1]:
        res -= len(_labels) / len(label) * entropy(_labels)
    return res

def one_split_ID3(feature, label):
    best_entropy = entropy(label)
    best_dimension = 0
    min_ent = 1
    for line in range(len(feature[0])):
        temp = 0
        ret = split(feature, label, line)
        for _labels in ret[1]:
            temp += len(_labels) / len(label) * entropy(_labels)
        if temp < min_ent:
            min_ent = temp
            best_dimension = line
    best_entropy -= min_ent
    return best_entropy, best_dimension

def one_split_C4_5(feature, label):
    best_entropy = 0
    best_dimension = 0
    for line in range(len(feature[0])):
        ret = split(feature, label, line)
        temp_ = np.sum([-len(v) / len(feature) * np.log2(len(v) / len(feature)) for v in ret[0]])
        if temp_ == 0:
            temp = 0
        else:
            temp = Gain(feature, label, line) / temp_
        if best_entropy < temp:
            best_entropy = temp
            best_dimension = line
    return best_entropy, best_dimension

def Gini(label):
    '''
    计算基尼值。
    '''
    _label = np.array(label).flatten()
    counter = Counter(_label)
    gini = 1
    for x in counter.values():
        gini -= (x / len(label)) ** 2
    return gini

def Gini_index(feature, label, dimension, value):
    '''
    计算基尼指数。为减少计算量与正文有所不同，只根据在某属性下
    是否取 value 划分为两个子集计算基尼值期望。
    '''
    d = np.c_[feature, label]
    d_1 = np.array(list(filter(lambda x : x[dimension] == value, d)))
    d_2 = np.array(list(filter(lambda x : x[dimension] != value, d)))
    return len(d_1) / len(d) * Gini(d_1[:, -1]) + len(d_2) / len(d) * Gini(d_2[:, -1])

def one_split_CART(feature, label):
    feature = np.array(feature)
    best_entropy, best_dimension, best_value = 1, 0, 0
    for line in range(len(feature[0])):
        for value in np.unique(feature[:, line]):
            gini_index = Gini_index(feature, label, line, value)
            if best_entropy > gini_index:
                best_entropy = gini_index
                best_dimension = line
                best_value = value
    return best_entropy,best_dimension,best_value

def best_split(D, A, split_func):
    '''
    进行一次当前子问题的最优划分。
    '''
    feature, new_dim = [], []
    for selected in A:
        feature.append(np.array(D)[:, selected])
        new_dim.append(selected)
    feature =  np.vstack(feature).T
    label = np.array(D)[:, -1]
    ret = split_func(feature, label)
    return new_dim[ret[1]]

# 树结点类
class Node:
    def __init__(self, isLeaf=True, label=-1, index=-1):
        self.isLeaf = isLeaf # isLeaf表示该结点是否是叶结点
        self.label = label # label表示该叶结点的label（当结点为叶结点时有用）
        self.index = index # index表示该分支结点的划分属性的序号（当结点为分支结点时有用）
        self.children = {} # children表示该结点的所有孩子结点，dict类型，方便进行决策树的搜索
        
    def addNode(self, val, node):
        self.children[val] = node #为当前结点增加一个划分属性的值为val的孩子结点

# 决策树类

class DTree:
    def __init__(self, split_func = one_split_ID3):
        self.tree_root = None #决策树的根结点
        self.possible_value = {} # 用于存储每个属性可能的取值
        self.split_func = split_func
    
        
    '''
    TreeGenerate函数用于递归构建决策树，伪代码参照课件中的“Algorithm 1 决策树学习基本算法”
    '''
    def TreeGenerate(self, D, A):
        
        # 生成结点 node
        node = Node()
        
        
        
        # if D中样本全属于同一类别C then
        #     将node标记为C类叶结点并返回
        # end if
        if len(np.unique(D[:, -1])) == 1:
            node.isLeaf = True
            node.label = D[0, -1]
            return node
        
        
        
        # if A = Ø OR D中样本在A上取值相同 then
        #     将node标记叶结点，其类别标记为D中样本数最多的类并返回
        # end if
        if len(A) == 0 or not False in (np.array([len(np.unique(D[:, x])) for x in A]) == np.ones(len(A))):
            node.isLeaf = True
            counter = np.array(list(Counter(D[:, -1]).items()))
            node.label = counter[np.argmax(counter[:, -1])][0]
            return node
        
        
        
        # 从A中选择最优划分属性a_star
        # （选择信息增益最大的属性，用到上面实现的best_split函数）
        best_feature = best_split(D, A, self.split_func)
        
        
        
        # for a_star 的每一个值a_star_v do
        #     为node 生成每一个分支；令D_v表示D中在a_star上取值为a_star_v的样本子集
        #     if D_v 为空 then
        #         将分支结点标记为叶结点，其类别标记为D中样本最多的类
        #     else
        #         以TreeGenerate(D_v,A-{a_star}) 为分支结点
        #     end if
        # end for
        #subD, subA = split(D[:, :-1], D[:, -1], best_feature)
        for value in self.possible_value[best_feature]:
            D_v = np.array(list(filter(lambda x : len(x) != 0, [D[x] if D[x, best_feature] == value else [] for x in range(len(D))])))
            if(len(D_v) == 0):
                node.isLeaf = False
                counter = np.array(list(Counter(D[:, -1]).items()))
                node.addNode(value, Node(label = counter[np.argmax(counter[:, -1])][0]))
            else:
                node.isLeaf = False
                node.index = best_feature
                A_v = list(filter(lambda x : x != best_feature, A))
                node.addNode(value, self.TreeGenerate(D_v, A_v))  
        return node
    
    
    
    
    '''
    train函数可以做一些数据预处理（比如Dataframe到numpy矩阵的转换，提取属性集等），并调用TreeGenerate函数来递归地生成决策树
    '''
    def train(self, D):
        D = np.array(D) # 将Dataframe对象转换为numpy矩阵（也可以不转，自行决定做法）
        A = range(D.shape[1] - 1) # 属性集A
        
#         #记下每个属性可能的取值
        for every in A:
            self.possible_value[every] = np.unique(D[:, every])
        
        self.tree_root = self.TreeGenerate(D, A) # 递归地生成决策树，并将决策树的根结点赋值给self.tree_root
    
    
    
    
    '''
    predict函数对测试集D进行预测， 并输出预测准确率（预测正确的个数 / 总数据数量）
    '''
    def predict(self, D):
        D = np.array(D) # 将Dataframe对象转换为numpy矩阵（也可以不转，自行决定做法）
        
#         #对于D中的每一行数据d，从当前结点x=self.tree_root开始，当当前结点x为分支结点时，
#         #则搜索x的划分属性为该行数据相应的属性值的孩子结点（即x=x.children[d[x.index]]），不断重复，
#         #直至搜索到叶结点，该叶结点的label就是数据d的预测label
        result = []
        for data in D:
            nw_node = self.tree_root
            while not nw_node.isLeaf:
                for child_feature, child_node in nw_node.children.items():
                    if child_feature == data[nw_node.index]:
                        nw_node = child_node
                        break
            result.append(nw_node.label)
        # print acc
        correct_sum = np.sum(np.array(result) == np.array(D)[:, -1])
        print(f"accuracy: {correct_sum / len(D) : .3}")
        return result        

train_frame = pd.read_csv('train_titanic.csv')
train_data = np.array(train_frame)
np.random.shuffle(train_data)
dt = DTree()

# 构建决策树
dt.train(train_data)

# 利用构建好的决策树对测试数据集进行预测，输出预测准确率（预测正确的个数 / 总数据数量）
test_frame = pd.read_csv('test_titanic.csv')
test_frame = test_frame[['Sex', 'sibsp', 'Parch', 'Pclass', 'Survived']]

test_data = np.array(test_frame)
result = dt.predict(test_data)
print(np.array(result[30:50]))
print(np.array(test_frame[30:50])[:, -1])
print(np.array(result[30:50]) == np.array(test_frame[30:50])[:, -1])
```