# 决策树

## 写在前面

决策树这东西，应该是最接近人类的线性逻辑推断方式的算法之一，或者说深度优先搜索本来就是一种惯用的思维模式，所以这算法其实是相当符合直觉的。

我们不妨来仔细研究一下人在对具有某些属性的未知事物作出经验判断的过程。首先可以肯定的是，判断的基本依据是我们曾经获得的知识，然后根据我们对未知物的各属性的了解程度为序，先从我们最了解的属性入手，抽丝剥茧缩小问题范围，最终确定一个最有可能的答案。比如我们遇到一个人，他每天都打游戏不认真听课，还经常错过各种重要活动，那么我们会认为这个人成绩不好的可能性是很高的，哪怕他可能成绩很好。

实际上决策树很大程度上就是在模拟这一过程，「判断的基本依据」在决策树中就是树的具体点边结构（抽象知识），其中「未知物的各属性名」即结点值，「未知物的某属性的取值」是边权，而「对未知物各属性了解程度」就是信息熵的表征，沿着这颗树从根走到叶子的一条路径就是关于某未知物的一个论断。

## 决策树的组成

### 前置概念

拿一张图来说省事些也好理解些。

![Screenshot-from-2022-11-06-17-11-29.png](http://image.tjzfile.xyz/images/2022/11/06/Screenshot-from-2022-11-06-17-11-29.png)

* 属性 $a$：第一行的除最后一列的内容。
* 属性取值 $a_v$：一种属性的可能取值集合是该属性所在列的不重复内容集合（除第一行），每个属性 $a$ 有一组可能的取值 $\{a_v\}_{v = 1}^{V}$。
* 类别：第一行最后一列的内容。
* 类别取值：最后一列除第一行的内容。
* 问题：在一组数据集 $D$（即上图中的某些行的集合）中，找到某种规律判定每个数据行的类别。
* 问题范围：问题对应数据集的大小 $|D|$。

### 决策树构造

* 决策树可能是多叉树（ID3/C4.5）也可能是二叉树（CART），但是都基于树这种基本数据结构，具有结点和边。
* 此外，这棵树是多模态的，它的叶子和非叶子所带信息种类是不同的。
  * 每个非叶子节点对应于某个属性，对于该属性的每一种可能取值引出一条边生成子树，这个子树代表着在该取值条件限定下的被缩小的问题范围。
  * 那么，根节点代表着总问题。
  * 每个叶子节点都是不可缩小的问题范围，在这个范围内决策树的知识无法再将其继续缩小（有三种情况），因此在此处决策树必须给出关于该问题的一个论断，该论断确定这叶子对应的类别。
* 这棵树的某条边代表这条边对应父节点属性的一个取值。

![Screenshot-from-2022-11-06-16-02-44.png](http://image.tjzfile.xyz/images/2022/11/06/Screenshot-from-2022-11-06-16-02-44.png)

> 注：
> （1）每一条从根节点到叶子的路径表示一个论断，即依次满足路径上属性取值的未知物为叶子类别。
> （2）决策树既可以做分类又可以做回归，因为如果把决策树形式化为图形，它可以表示一条折线。
> （3）决策树的训练集跟其他机器学习算法没什么不同，也可以看做特征向量。

这是决策树的一个相对具体的例子，应该与上文的叶子和非叶子节点、边进行对应比较理解。

![Screenshot-from-2022-11-06-16-10-07.png](http://image.tjzfile.xyz/images/2022/11/06/Screenshot-from-2022-11-06-16-10-07.png)

## 分析问题

* 在了解了决策树的原理和构造后，我们便要着手考虑如何生成（学习）一颗决策树。显然我们希望生成的决策树尽可能最优，即它应该与训练数据矛盾较小同时泛化能力较强。
* 当然，我们的起始问题是根节点，已知的信息是训练集。

不妨先分析一下需要解决的问题：

（1）如何选择一个非叶子节点的属性才是合理的？
（2）何时判定一个节点为叶子节点？
（3）如何根据（1）（2）建立树形结构？

为了解决问题（1），我们引入了信息熵和基尼系数作为判断依据。对于问题（2），我们只需要解决三种情况。

（3）问题是简单的，这就是数据结构课上的 DFS 建树，只不过条件更复杂。这条件简单地说，就是在 DFS 生成每个节点时都考虑（1）（2）。

### 选择非叶子节点属性

#### 信息增益和信息增益比

* 选择非叶子节点属性的依据基于这样的信息论理论：信息熵越大，信息量越大，未知物的不确定性越大。如果信息熵减小，那么未知物的不确定性减小。

首先明确我们选择一个非叶子节点属性的目标，就是要让子问题的**不确定性减少**地越多越好。具体的说就是要使得根据该属性取值划分的子问题范围的数据的类别尽可能一致，即**信息熵减小**。

能够让子问题的不确定性减少得最多的属性，一定是当前最有力的判据。举个例子。

>还是用上文的例子，我们选取两个属性：打游戏的频率和刷牙的频率。当我们得知一个人天天打游戏时，我们几乎可以断定他成绩不好；但如果我们只知道他每天不刷牙，那么认为他成绩不好是不如前者恰当的。如果你知道一些人的这两种属性而需要判断他们的成绩，那么前者一定是重要一些的。

在这个例子中，打游戏频率这个属性让难以分辨的人群得到了一个比较清晰的分类（打游戏的更多在成绩好类中，不打的更多在成绩好中），这就是不确定性的减少。

所以，我们只需要度量**用不同属性划分训练集得到的子问题信息熵总和并与划分前的信息熵作差**，即可得到不确定性减少的数学表达，这就是信息增益。而信息增益最大的属性，就是使得不确定性减少最多的属性。

---

解释地差不多了，那么就着手将其形式化为数学表达。注意其实信息熵的原本定义跟决策树的是有一点点不一样的。

* 信息量

一个随机事件的信息量，与它发生概率的大小有关，发生概率越大该事件确定性越大，信息量越小。在决策树中，我们把数据集中不同类别出现的频率作为概率的度量。

假设 $D$ 中有 $n$ 个不同类别的子集 $\{C_k\}_{k = 1}^n$，那么

$$
p_k = \dfrac{|C_k|}{|D|}
$$

$$
I_k = -\log_2 p_k
$$

* 信息熵

信息熵是样本信息量的期望，是整个样本不确定性程度的度量，信息熵越大，不确定性越大。如果数据集中各个类别出现的频率都差不多，那么这个数据集的不确定程度就很大。

$$
Ent(D) = -\sum_{k = 1}^n p_k\log_2 p_k
$$

![Screenshot-from-2022-11-06-19-14-15.png](http://image.tjzfile.xyz/images/2022/11/06/Screenshot-from-2022-11-06-19-14-15.png)

> 注：
> （1）$Ent(D)$ 越小，样本纯度越高。
> （2）若 $p_k = 0$，则 $p_k\log_2 p_k = 0$。
> （3）信息量具有可加性。

* 信息增益

属性 $a$ 有 $V$ 个可能的取值，即 $\{a_v\}_{v = 1}^V$，记在数据集 $D$ 上属性 $a$ 取值 $a_v$ 的子集为 $D_v$，则信息增益

$$
Gain(D, a) = \underset{划分前的信息熵}{\underline{Ent(D)}} - \underset{划分后的信息熵}{\underline{\sum_{v = 1}^{V}\dfrac{|D_v|}{|D|}Ent(D_v)}}
$$

我们在所有属性中取信息增益最大的作为当前节点的取值即可，ID3算法使用了信息增益。

> 注：
> （1）信息熵和信息增益计算的全过程实际上并不关心属性具体是什么，只关心训练集上属性不同取值的**类别比例**。
> （2）信息增益对取值数目多的属性存在偏好，因为这属性对问题刻画更详细，信息熵更有可能较小。

* 信息增益率

信息增益率希望消除信息增益对取值数目多的属性的偏好，于是它添加了一个关于属性 $a$ 可取值的惩罚项 $IV(a)$，其实就是属性集合的信息熵（上面那个是类别集合的信息熵），它越大，类别集合对减熵的贡献越小。

$$
IV(a) = -\sum_{v = 1}^V\dfrac{|D_v|}{|D|}\log_2\dfrac{|D_v|}{|D|}
$$

$$
Gain\_ratio(D, a) = \dfrac{Gain(D, a)}{IV(a)}
$$

和增益一样，我们需要在所有属性中选择增益率最大的。C4.5算法使用了信息增益率。

> 注：信息增益和增益率都是非负的，参考[这里](https://stackoverflow.com/questions/3289589/can-the-value-of-information-gain-be-negative#:~:text=Sure%20it%20can.%20Information%20gain%20is%20just%20the,in%20either%20direction--it%20can%20be%20positive%20or%20negative.)。

#### 基尼系数

* 基尼系数本来是用来度量样本均匀程度的，比如贫富差距啥的，样本越不均匀基尼系数越大。

稍微转换一下思维，样本越不平均，不就相当于样本信息熵越小吗，实际上它们的函数图像也很像。所以基尼系数也可以用同样的思路解决选择非叶子节点属性的问题。

* 基尼值

基尼值是度量样本纯度最常用的一种指标，下式符号含义与上文一致。

$$
Gini(D) = 1 - \sum_{k = 1}^n p_k^2
$$

* 基尼系数



### 判定叶子节点

## 算法步骤