# 线性模型

线性模型是一种非概率模型，它试图学得一个通过属性的**线性组合**来进行预测的函数

$$
f(\pmb x) = \pmb\omega^T \pmb x + b
$$

其中 $\pmb\omega = (\omega_1;\omega_2;\cdots;\omega_d)$ 为参数向量，$\pmb x$ 为特征向量。

## 线性回归

* 线性回归的一维形式在中学阶段已经学过，但是当时多半只给了个结论。

对于一维输入的线性模型

$$
f(x_i) = \omega x_i + b
$$

我们希望平方损失函数（或欧式距离）

$$
L(Y,f(X);\omega, b) = \sum_{i = 1}^m(f(x_i) - y_i)^2
$$

达到最小，使得模型对真实情况 $y_i$ 有良好的拟合

$$
(\omega^*, b^*) = \underset{(\omega, b)}{\arg\min}\sum_{i=1}^m(f(x_i) - y_i)^2
$$

这个式子的意思是找到一个 $(\omega, b) = (\omega^*, b^*)$ 使得训练结果与真实值的欧式距离之和最小。

对 $\omega, b$ 求偏导

$$
\dfrac{\partial L}{\partial\omega} = 2\left(\omega\sum_{i=1}^m x^2_i - \sum_{i=1}^m(y_i - b)x_i\right)
$$

$$
\dfrac{\partial L}{\partial b} =2\left( mb - \sum_{i = 1}^m(y_i - \omega x_i) \right)
$$

取偏导为 $0$ 求极值解得

$$
\omega = \dfrac{\sum_{i=1}^m y_i(x_i - \overline{x})}{\sum_{i=1}^m x_i^2 - \frac{1}{m}(\sum_{i=1}^m x_i)^2}
$$

$$
b = \dfrac{1}{m}\sum_{i=1}^m(y_u - \omega x_i)
$$

* 对于多元线性回归，需要用到矩阵求导（因为变量 $\pmb\omega$ 是一个向量，所以其实只是对一个列向量求导罢了），大体上与正常求导一样，说几点重要的。

>（1）矩阵求导的四则运算法则与实函数的求导法则形式相同
>
>（2）矩阵求导和实函数求导一样具有线性性质
>
>（3）关于转置矩阵有性质
>
>$$
>\dfrac{\partial \pmb x^T \pmb a}{\partial\pmb x}=\dfrac{\partial\pmb a^T\pmb x}{\partial\pmb x}=\pmb a
>$$
>
>$$
>\dfrac{\partial\pmb x^T\pmb x}{\partial\pmb x}=2\pmb x
>$$
>
>$$
>\dfrac{\partial\pmb x^T \pmb A \pmb x}{\partial\pmb x}=\pmb{Ax} + \pmb A^T \pmb x
>$$
>
>

有关以上性质的证明和矩阵求导参见[矩阵求导公式的数学推导](https://zhuanlan.zhihu.com/p/273729929)（恐怖如斯的 $\LaTeX$ 公式）。

同样的，对于多元线性回归的情况

$$
f(\pmb x_i) = \pmb\omega^T \pmb x_i + b
$$

平方损失函数可以写成

$$
\begin{aligned}
    L(Y,f(X);\pmb\omega, b)
    &= \sum_{i = 1}^m(f(\pmb x_i) - y_i)^2\\
    &= \sum_{i = 1}^m(\pmb\omega^T \pmb x_i + b - y_i)^2\\
\end{aligned}
$$

不难发现其中有一实数 $b$ 导致难以处理，故我们设一 $\hat{\pmb\omega} = (\pmb \omega; b)$，并令 $\pmb y = (y_1; y_2;\cdots; y_d)$，且

$$
\textbf X = 
\begin{bmatrix}
    x_{11} & x_{12} &\cdots& x_{1d} & 1\\
    x_{11} & x_{12} &\cdots& x_{1d} & 1\\
    \vdots & \vdots &\ddots& \vdots & \vdots\\
    x_{d1} & x_{d2} &\cdots& x_{dd} & 1\\
\end{bmatrix}
= \
\begin{bmatrix}
   \pmb x_1^T & 1\\
   \pmb x_2^T & 1\\
   \vdots & \vdots\\
   \pmb x_d^T & 1\\
\end{bmatrix}
$$

则有

$$
L(Y,f(X);\pmb\omega, b) = (\pmb y - \textbf X\hat{\pmb\omega})^T(\pmb y - \textbf X\hat{\pmb\omega})
$$

对 $\hat{\pmb\omega}$ 求导（求导第三条性质）得到

$$
\dfrac{\partial L}{\partial{\hat{\pmb\omega}}} = 2\textbf X^T(\textbf X\hat{\pmb\omega} - \pmb y)
$$

当 $\textbf X^T\textbf X$ 为满秩矩阵时，存在逆矩阵 $(\textbf X^T\textbf X)^{-1}$，此时 $\dfrac{\partial L}{\partial{\hat{\pmb\omega}}} = 0$ 有解

$$
\hat{\pmb\omega}^* = (\textbf X^T\textbf X)^{-1}\textbf X^T y
$$

令 $\hat{\pmb x_i} = (\pmb x_i;1)$ 则此线性回归模型的最小二乘近似为

$$
f(\hat{\pmb x_i}) = \hat{\pmb x_i}^T\hat{\pmb\omega}^* = \hat{\pmb x_i}^T(\textbf X^T\textbf X)^{-1}\textbf X^T y
$$

但一般情况下，$\textbf X^T\textbf X$ 非满秩，存在多个解需要进行迭代优化，而随机梯度下降或牛顿迭代法可能遭遇多个极小值，这时就需要引入正则项判断解的优良性。

### 线性回归的求解方法

## 广义线性回归

* 通过对线性模型 $y = \pmb \omega^T\pmb x + b$ 的非线性变换，我们可以将线性回归转换成非线性回归，并使用线性回归的方法解决（反之依然）。

建立一个线性模型到非线性模型的映射 $g(\cdot)$，称为关联函数，令

$$
y' = g^{-1}(\pmb \omega^T\pmb x + b)
$$

即可得到一个非线性模型 $y'$，这个 $y'$ 可以通过 $g(y') = \pmb \omega^T\pmb x + b$ 转换成线性模型预测和拟合。

比如当 $g(\cdot) = \ln(\cdot)$ 时，线性模型变为对数线性回归 

$$
\ln y = \pmb \omega^T\pmb x + b
$$

这样一来，**就可以方便地对各种非线性模型应用最优化方法了**。

### 对数几率回归

* 对数几率回归说是回归，其实是用来分类的（准确地说是用类似回归的方法做分类），其中对数几率四字揭示了拟合函数的性质。

正常来想，用数值函数做二分类，当然是用二值函数之类的，像阶跃函数这种：

$$
y = \left\{
\begin{aligned}
    0,\ z < 0\\
    0.5,\ z = 0\\
    1,\ z > 0
\end{aligned}
\right.
\\\ \\
z = \pmb \omega^T\pmb x + b
$$

然后做一个从线性模型到这个二值（或者应该是三值）函数的映射，就可以容易地解决了。

但是实际上这类函数就难以做数值优化，或者说难以放进机器学习里面去用，因为它**不连续**，没法迭代。于是我们考虑一形状十分相似的函数

$$
y = \dfrac{1}{1+ e^{-z}}
$$

![funtion1.png](http://image.tjzfile.xyz/images/2022/09/15/funtion1.png)

它具有极其优秀的性质（这也注定了它在机器学习中的广泛应用）：

* 无穷阶可微
* 处处连续
* 单调递增且有界

带入 $z$ 并整理一下得到

$$
\ln\dfrac{y}{1 - y} = \pmb \omega^T\pmb x + b
$$

这个 $\dfrac{y}{1 - y}$ 就可以做一些有趣的解读了，我们不妨假设 $y$ 代表 $\pmb x$ 为正例的概率，而 $1 - y$ 代表反例的（正是得益于这种函数的单调连续有界性使得我们能够这样假设），那么则称