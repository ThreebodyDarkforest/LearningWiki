# 线性模型

线性模型是一种非概率模型，它试图学得一个通过属性的**线性组合**来进行预测的函数

$$
f(\pmb x) = \pmb\omega^T \pmb x + b
$$

其中 $\pmb\omega = (\omega_1;\omega_2;\cdots;\omega_d)$ 为参数向量，$\pmb x$ 为特征向量。

## 线性回归

* 线性回归的一维形式在中学阶段已经学过，但是当时多半只给了个结论。

对于一维输入的线性模型

$$
f(x_i) = \omega x_i + b
$$

我们希望平方损失函数（或欧式距离）

$$
L(Y,f(X);\omega, b) = \sum_{i = 1}^m(f(x_i) - y_i)^2
$$

达到最小，使得模型对真实情况 $y_i$ 有良好的拟合

$$
(\omega^*, b^*) = \underset{(\omega, b)}{\arg\min}\sum_{i=1}^m(f(x_i) - y_i)^2
$$

这个式子的意思是找到一个 $(\omega, b) = (\omega^*, b^*)$ 使得训练结果与真实值的欧式距离之和最小。

对 $\omega, b$ 求偏导

$$
\dfrac{\partial L}{\partial\omega} = 2\left(\omega\sum_{i=1}^m x^2_i - \sum_{i=1}^m(y_i - b)x_i\right)
$$

$$
\dfrac{\partial L}{\partial b} =2\left( mb - \sum_{i = 1}^m(y_i - \omega x_i) \right)
$$

取偏导为 $0$ 求极值解得

$$
\omega = \dfrac{\sum_{i=1}^m y_i(x_i - \overline{x})}{\sum_{i=1}^m x_i^2 - \frac{1}{m}(\sum_{i=1}^m x_i)^2}
$$

$$
b = \dfrac{1}{m}\sum_{i=1}^m(y_u - \omega x_i)
$$

* 对于多元线性回归，需要用到矩阵求导（因为变量 $\pmb\omega$ 是一个向量，所以其实只是对一个列向量求导罢了），大体上与正常求导一样，说几点重要的。

>（1）矩阵求导的四则运算法则与实函数的求导法则形式相同
>
>（2）矩阵求导和实函数求导一样具有线性性质
>
>（3）关于转置矩阵有性质
>
>$$
>\dfrac{\partial \pmb x^T \pmb a}{\partial\pmb x}=\dfrac{\partial\pmb a^T\pmb x}{\partial\pmb x}=\pmb a
>$$
>
>$$
>\dfrac{\partial\pmb x^T\pmb x}{\partial\pmb x}=2\pmb x
>$$
>
>$$
>\dfrac{\partial\pmb x^T \pmb A \pmb x}{\partial\pmb x}=\pmb{Ax} + \pmb A^T \pmb x
>$$
>
>

有关以上性质的证明和矩阵求导参见[矩阵求导公式的数学推导](https://zhuanlan.zhihu.com/p/273729929)（恐怖如斯的 $\LaTeX$ 公式）。

同样的，对于多元线性回归的情况

$$
f(\pmb x_i) = \pmb\omega^T \pmb x_i + b
$$

平方损失函数可以写成

$$
\begin{aligned}
    L(Y,f(X);\pmb\omega, b)
    &= \sum_{i = 1}^m(f(\pmb x_i) - y_i)^2\\
    &= \sum_{i = 1}^m(\pmb\omega^T \pmb x_i + b - y_i)^2\\
\end{aligned}
$$

不难发现其中有一实数 $b$ 导致难以处理，故我们设一 $\hat{\pmb\omega} = (\pmb \omega; b)$，并令 $\pmb y = (y_1; y_2;\cdots; y_d)$，且

$$
\bold X = 
\begin{bmatrix}
    x_{11} & x_{12} &\cdots& x_{1d} & 1\\
    x_{11} & x_{12} &\cdots& x_{1d} & 1\\
    \vdots & \vdots &\ddots& \vdots & \vdots\\
    x_{d1} & x_{d2} &\cdots& x_{dd} & 1\\
\end{bmatrix}
= \
\begin{bmatrix}
   \pmb x_1^T & 1\\
   \pmb x_2^T & 1\\
   \vdots & \vdots\\
   \pmb x_d^T & 1\\
\end{bmatrix}
$$

则有

$$
L(Y,f(X);\pmb\omega, b) = (\pmb y - \bold X\hat{\pmb\omega})^T(\pmb y - \bold X\hat{\pmb\omega})
$$

对 $\hat{\pmb\omega}$ 求导（求导第三条性质）得到

$$
\dfrac{\partial L}{\partial{\hat{\pmb\omega}}} = 2\bold X^T(\bold X\hat{\pmb\omega} - \pmb y)
$$

当 $\bold X^T\bold X$ 为满秩矩阵时，存在逆矩阵 $(\bold X^T\bold X)^{-1}$，此时 $\dfrac{\partial L}{\partial{\hat{\pmb\omega}}} = 0$ 有解

$$
\hat{\pmb\omega}^* = (\bold X^T\bold X)^{-1}\bold X^T y
$$

令 $\hat{\pmb x_i} = (\pmb x_i;1)$ 则此线性回归模型的最小二乘近似为

$$
f(\hat{\pmb x_i}) = \hat{\pmb x_i}^T\hat{\pmb\omega}^* = \hat{\pmb x_i}^T(\bold X^T\bold X)^{-1}\bold X^T y
$$

但一般情况下，$\bold X^T\bold X$ 非满秩，随机梯度下降可能遭遇多个极小值，这时就需要引入正则项判断解的优良性。