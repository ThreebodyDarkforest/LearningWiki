# 机器学习的基本知识

## 机器学习要解决的问题

* 分类：估计离散高维函数的参数，预测特定输入特征的输出。
* 回归：估计连续高维函数的参数，预测特定输入特征的输出。

## 机器学习的分类

### 前置技能

* 输入空间和输出空间：输入和输出所有可能的取值的集合，分别记为 $\mathcal{X},\mathcal{Y}$。
* 特征空间：特征向量所在的空间，可能与输入空间相同，也可能是处理过的输入组成的。
* 假设空间：模型预测所在的空间，是一个输入空间到输出空间的映射的集合，记为 $\mathcal{H} = \{f|\mathcal{X}\rightarrow\mathcal{Y}\}$，它**包含了所有可能的模型的目标函数**。
* 概率模型和非概率模型：设输入 $X$ 输出 $Y$，则概率模型由 $P(Y|X)$ 表示，非概率模型由 $Y=f(X)$ 表示。

### 监督学习

* 监督学习是指从**有标记**的数据中学习预测模型的机器学习问题。

* 训练过程
>监督学习在训练时的输入为
>
>$$
T=\{(\pmb x_i, y_i)\}^m_{i=1}
$$
>
>称为**训练集**
>
>其中
>
>$$
\pmb x_i = (x_{i1}; x_{i2};\cdots;x_{id})\in \mathcal{X} \subset R^d
$$
>
>称为**特征向量**，对应的 $y_i\in \mathcal{Y}$ 则为**标记**

监督学习的训练过程中，产生一个**模型**，它在假设空间上，定义为 $\mathcal{H} = \{f|\mathcal{X}\rightarrow\mathcal{Y}\}$。

这个模型是对真实情况模型的一个估计，模型可分为**概率模型和非概率模型**。

* 预测过程
>监督学习的模型输入一组
>
>$$
\{\pmb x_i\}_{i=1}^m\in \mathcal{X}
$$
>
>并给出
>
>$$
\{y_i\}_{i=1}^m\in \mathcal{Y}
$$
>
>的一个预测值

监督学习分为学习和预测两个过程，图示如下：

``` mermaid
graph LR
训练集--> sys1(学习系统) --> sys2(模型)
测试集-->sys3(预测系统) --- sys2
sys3 --> 输出
```

### 无监督学习

* 无监督学习是指从**无标记**的数据中学习预测模型的机器学习问题。

它的运作模式与监督学习基本一致，但是训练时的输入

$$
T =\{x_i\}_{i=1}^m
$$

是无标注的。

### 强化学习

* 智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。
* 本质是**马尔可夫决策过程**的一个机器学习模型。

## 机器学习的关键要素

### 数据

* 数据集一般分为训练集和测试集，差不多就是字面意思。
* **机器学习假设所有输入样本服从同一个未知的分布**$\mathcal{D}$，这个分布多半是高维非线性的。

### 模型

* 模型的假设空间包含所有可能的条件概率分布或决策函数，以 $\mathcal{H}$ 表示假设空间，则有

$$
\mathcal{H} = \{f|Y = f_\theta(X),\theta\in R^n\}\ or\ \{P|P_\theta(Y|X),\theta\in R^n\}
$$

即 $\mathcal{F}$ 通常是一个由参数向量 $\theta$ 决定的函数族。

### 策略

* 这部分是（~~调包~~）最重要的内容。
* 在训练过程中需要一个参数给出训练结果与真实情况间的差异，并希望最小化这个差异，这个参数通常通过**损失函数**给出。

一般有以下几种损失函数：

（1）$0-1$ 损失函数：

$$
L(Y,f(X)) = \left\{
\begin{aligned}
    1, Y\neq f(X)\\
    0, Y = f(X)
\end{aligned}
\right.
$$

（2）平方损失函数：

$$
L(Y, f(X)) = (Y - f(X))^2
$$

（3）绝对损失函数：

$$
L(Y, f(X)) = |Y - f(X)|
$$

（4）对数损失函数：

$$
L(Y, f(X)) = -\log P(Y|X)
$$

对于概率模型，若设**训练过程中**输入与输出的随机变量 $\pmb x,y$ 遵循一个联合概率分布 $P(\pmb x, y)$，则根据概率论的知识可以知道损失函数的期望

$$
E(f;\mathcal D)=\int_{\mathcal{X} \times\mathcal{Y}}L(y,f(\pmb x))p(\pmb x, y)d\pmb xdy
$$

但是这一期望在训练过程中是不可计算的，因为训练过程中联合概率分布 $P(\pmb x, y)$ 的形式是未知的，这一问题被称为**病态问题**。

因此我们通常选用**经验风险**来对期望损失进行近似

$$
E(f;D) \sim \dfrac{1}{m}\sum_{i=1}^m L(y_i, f(\pmb x_i))
$$

经验风险对期望损失的近似的误差随着数据量的增大而趋于零，这是某种程度上的极大似然估计。

但是一般来说我们并没有那么多数据来进行训练，且大样本若控制不当极易导致**过拟合**，因此常常通过对经验风险添加正则项修正为**结构风险**，转而优化结构风险。

$$
\hat f = \min_{f\in \mathcal{H}}\{\dfrac{1}{m}\sum_{i=1}^m L(y_i, f(\pmb x_i)) + \lambda J(f)\}
$$

其中 $J(f)$ 表示模型的复杂度。

### 算法

* 传统的机器学习算法几乎都可归结为最优化问题，解决最优化问题的算法相当多。
* 最优化算法往往在准确性与性能上作出权衡。