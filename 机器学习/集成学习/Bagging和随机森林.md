# Bagging 和随机森林

## Bagging

Bagging 没什么特别的，其实就是通过 Bootstrap 采样法对训练集进行采样产生若干子集，然后以不同子集为新训练集训练多个基学习器，最后结合这些学习器。

具体来说，个体学习器的产生采用并行化方法，用 Bootstrap 从含 $m$ 个样本的训练集中采样 $T$ 个含 $m$ 个样本的数据集作为新训练集并在每个新训练集上训练基学习器，随后用简单投票法（分类）或简单平均法（回归）得到最终分类器。

![QQ20230124202610.png](http://image.tjzfile.xyz/images/2023/01/24/QQ20230124202610.png)

> 注：（1）从偏差-方差分解的角度看，Bagging 方法可以降低方差，在不剪枝的决策树、神经网络等易受样本影响的学习器上效果更好。
> （2）Bagging 的时间复杂度低（与单个基学习器的训复杂度同阶），可以不经修改地适用于多分类和回归任务。

## 随机森林

随机森林是 Bagging 的一个实现，它采用决策树作为基学习器。除此之外，它在每个节点进行最优属性选择时，首先在可选属性集中随机选取 $k$ 个（假设属性集大小 $d$），再在这 $k$ 个属性中选取最优属性用于划分。

* 若 $k = d$，则该决策树（基学习器）的构建与传统决策树相同。
* 若 $k = 1$，则该决策树随机选取一个属性用于划分。
* 一般情况下取 $k = \log_2 d$。