# 集成学习

集成学习通过构建并结合多个学习器来完成学习任务，它能通过组合多个弱学习器来构建一个强学习器，它基于如下定理

> 弱可学习是强可学习的充要条件。

它的结构如下

![QQ20230123182557.png](http://image.tjzfile.xyz/images/2023/01/23/QQ20230123182557.png)

其中**个体学习器**可以有多种不同的选择

* 若学习器都是同类（如都是感知机）的，则该集成称为同质集成，其中学习器称为基学习器。
* 若学习器是不同类的（如决策树、感知机、神经网络的混合），则改集成称为异质集成，其中学习器称为组件学习器。

**结合模块**也有多种不同的策略

* 平均法：用于数值型输出，有简单平均、加权平均等。
  * 简单平均
$$
H(\pmb x) = \dfrac{1}{M}\sum_{m = 1}^MG_m(\pmb x)
$$
  * 加权平均
$$
H(\pmb x) = \sum_{m = 1}^M\omega_mG_m(\pmb x),~ \sum_{m = 1}^M\omega_m = 1
$$
* 投票法：用于分类，有绝对多数投票、相对多数投票、加权投票等。设 $G_{m}^j\in\{0, 1\}$，若 $G_m(\pmb x) = c_j$ 则取 $1$，否则取 $0$，其中 $c_j$ 表示类别 $j$。并设 $n$ 为样本类别个数。
  * 绝对多数投票
$$
H(\pmb x) = \left\{
\begin{aligned}
    &c_j,&~\sum_{m = 1}^MG_{m}^j(\pmb x) > \dfrac{1}{2}\sum_{k = 1}^n\sum_{m = 1}^MG_m^k(\pmb x)\\
    &rejection,&~else
\end{aligned}
\right.
$$
  * 相对多数投票
$$
H(\pmb x) = c_{\underset{j}{\argmax}\sum_{m = 1}^MG_m^j(\pmb x)}
$$
  * 加权投票
$$
H(\pmb x) = c_{\underset{j}{\argmax}\sum_{m = 1}^M\omega_mG_m^j(\pmb x)}
$$
* 学习法：用于训练数据较多较复杂时，如 Stacking。

而对于不同集成方法，又有不同的**学习器生成策略**

* 并行化：个体学习器间不存在依赖和先后关系，可同时生成，如 Bagging 和随机森林。
* 串行化：个体学习器间存在依赖关系，后生成的学习器依赖之前生成的，比如 Boosting。

不难发现，集成结果的好坏与学习器和结合模块的选择有关

* 一般来说在同质集成下，随着学习器的数目的增加，集成的错误率将指数级下降，最终趋于 $0$。
* 不同的学习器应在同一特征空间的不同部分具有良好的拟合能力，以提高集成对整个特征空间的预测性能（学习器应做到准确性和多样性并存）。