# AdaBoost 算法

## Boosting

Boosting 意为促进、提升，在机器学习中指一类可将弱学习器提升为强学习器的方法。Boosting 是一种串行化集成学习方法，它通过改变每个学习器在训练集上不同部分的关注度来生成这些学习器，而每个新学习器依赖于上一个产生的学习器在训练数据集上的表现。

## AdaBoost

AdaBoost 是一种 Boosting 方法，它的基本思路如下

* 改变学习器在训练集上不同部分的关注度：提高上一个学习器错误分类样本的权重，降低分类正确的样本的权重，训练新的学习器。
* 弱分类器组合成强分类器：加权多数表决，学习器分类错误率越低所占权重越大。

> 注意：此处有两种权重——训练集不同样本的权重、学习器组合时的权重。

![QQ20230124194417.jpg](http://image.tjzfile.xyz/images/2023/01/24/QQ20230124194417.jpg)
![QQ20230124194423.jpg](http://image.tjzfile.xyz/images/2023/01/24/QQ20230124194423.jpg)

> 注：
> （1）弱学习器 $G_m(\pmb x)$ 的系数 $\alpha_m$ 表示 $G_m(\pmb x)$ 在最终分类器中的权重。若 $e_m\le 0.5$ 则 $\alpha_m \ge 0$，并且 $\alpha_m$ 随着 $e_m$ 的减小而增大。
> （2）被弱学习器 $G_m(\pmb x)$ 误分类的数据将在下一轮增大权重，分类正确的点则减小权重。
> （3）$\sum_{m = 1}^M\alpha_m\neq 1$，$f(\pmb x)$ 的符号确定分类结果，而 $f(\pmb x)$ 的值确定分类确信度。
> （4）$\sum_{i = 1}^n\omega_{mi} = 1, m = 1, 2,\cdots, M$
> （5）从偏差-方差分解的角度来看，AdaBoost 降低偏差，可用泛化性能相当弱的学习器构造出强学习器。